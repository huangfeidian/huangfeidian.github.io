<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ADMM - Spirits Away</title>
    <link rel="shortcut icon" type="image/x-icon" href="\favicon.ico">
    <link rel="stylesheet" href="http://spiritsaway.info/theme/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="http://spiritsaway.info/theme/font-awesome/css/font-awesome.min.css" type="text/css" />
    <link rel="stylesheet" href="http://spiritsaway.info/theme/css/niu2.css" type="text/css" />

    <link rel="canonical" href="http://spiritsaway.info/ai/admm.html" />
    
    <script type="text/javascript">window.onload=function(){};</script>
    <!--[if lt IE 9]>
        <script src="http://spiritsaway.info/theme/js/html5shiv.js"></script>
        <script src="http://spiritsaway.info/theme/js/respond.min.js"></script>
    <![endif]-->
  </head>
  <body> 
    <div id="body-header">
<div class="navbar navbar-inverse navbar-fixed-top">
<div class="col-md-12">
    <div class="navbar-header col-md-offset-2">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="http://spiritsaway.info">
            <i class="fa fa-home"></i>Spirits Away
        </a>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
        <ul class="nav navbar-nav">
            <li><a href="/about.html" title="about">
                <i class="fa fa-anchor"></i>关于</a>
            </li>
            <li><a href="/archives.html" title="archives">
                <i class="fa fa-archive"></i>存档</a>
            </li>
            <li><a href="/tag/" title="tags">
                <i class="fa fa-tag"></i>标签</a>
            </li>
        <!-- category dropdown list -->
        <li class="dropdown">
           <a href="#" class="dropdown-toggle" data-toggle="dropdown">
                <i class="fa fa-folder-open"></i>分类<b class="caret"></b>
           </a>
           <ul class="dropdown-menu">
               <li><a href="http://spiritsaway.info/ai/index.html">
                   <i class="fa fa-folder-close"></i>AI
                      (3)</a></li>
               <li><a href="http://spiritsaway.info/cpp/index.html">
                   <i class="fa fa-folder-close"></i>CPP
                      (4)</a></li>
               <li><a href="http://spiritsaway.info/math/index.html">
                   <i class="fa fa-folder-close"></i>Math
                      (3)</a></li>
           </ul>
        </li>
        <!--  self defined dropdown list -->
        </ul>
        
        <!-- right nav bar -->
        <ul class="nav navbar-nav navbar-right">
        <!-- google custom search -->
       </ul>
    </nav>
</div>
</div>    </div>
    
    <div id="body-content">
<div class="col-md-8 col-md-offset-2">
    <h1 id="content-heading">ADMM</h1>
</div>
<div id="niu2-left-container" class="col-md-6 col-md-offset-2 with-right-border">
    <div id="niu2-main-content">
        <h1 id="844962">扩展的拉格朗日乘子方法</h1>
<p><code>Augmented Lagrangrians and the Method of Multipliers</code>是凸优化问题的一种通用解法. 在凸优化中，我们一般会遇到如此形式的优化问题</p>
<div class="math">\begin{equation}\label{eq:oringinal}minimize \quad f(x) \quad w.r.t. \quad Ax=b\end{equation}</div>
<p><br />
上式经过拉格朗日乘子变换之后，变为了</p>
<div class="math">\begin{equation}\label{eq:normalLag}minimize \quad f(x)+y^{T} (Ax-b) \quad w.r.t. \quad y\ge 0\end{equation}</div>
<p><br />
而所谓的扩展拉格朗日形式则是在优化问题\ref{eq:normalLag}的基础上，增加了一个二次项，即</p>
<div class="math">\begin{equation}\label{eq:AugmentedLag}minimize \quad f(x)+y^{T} (Ax-b)+(\rho /2)\Vert Ax-b\Vert _{2}^{2} \quad w.r.t. \quad y\ge 0 \, \rho&gt;0\end{equation}</div>
<p><br />
在加上这个二次项之后，可以保证新的优化问题是强凸的，因此在可行集上只有一个极小值点。而且新问题仍然等价于原始的优化问题\ref{eq:oringinal}，因为任何满足原始限制条件的<span class="math">\(x\)</span>都会使二次项变为0，其中的<span class="math">\(\rho\)</span>是惩罚参数.此时的对偶函数可以表示为<span class="math">\(g_{\rho}(y)=\text{inf}_x L_{\rho}(x,y)\)</span>，而原有的优化问题转变为了<span class="math">\(\text{max}_y g_{\rho}(y)\)</span>。此问题的解法仍然是对偶上升法(<code>dual ascent</code>)，即迭代的进行如下的两步操作：</p>
<div class="math">\begin{equation}\label{eq:FinalLag}\begin{aligned}&amp;x^{k+1}:=\mathop{\argmin}_{x} L_{\rho}(x,y^k)\\&amp; y^{k+1}:=y^k+\rho (Ax^{k+1}-b)\end{aligned}\end{equation}</div>
<p><br />
这里之所以采取<span class="math">\(\rho\)</span>作为\ref{eq:FinalLag}中的更新步长，是出于如下考虑。在达到最优解的时候，我们可以得到如下条件：</p>
<div class="math">\begin{equation}Ax^{\star}-b=0 ,\qquad \nabla f(x^{\star})+A^Ty^{\star}=0\end{equation}</div>
<p><br />
根据定义，<span class="math">\(x^{k+1}\)</span>可以使<span class="math">\(L_{\rho}(x,y^k)\)</span>得到最小值。所以：</p>
<div class="math">\begin{equation}\begin{aligned}0&amp;=\nabla _{x}L_{\rho}(x^{k+1},y^k)\\&amp;=\nabla _x f(x^{k+1})+A^T (y^k+\rho (Ax^{k+1}-b))\\&amp;=\nabla _x f(x^{k+1})+A^Ty^{k+1}\end{aligned}\end{equation}</div>
<p><br />
因此，在此步长下，如果达到了最优解，则<span class="math">\(x,y\)</span>的值将不再变化。</p>
<p>在实践中，二次项的引入可能会造成一些麻烦，特别是在<span class="math">\(f(x)\)</span>可以分为多个部分的时候，二次项不可分。</p>
<h1 id="d20776">ADMM介绍</h1>
<p>ADMM的全称为<code>Alternating Direction Method of Multipliers</code>，其主要目的就是利用优化问题的可分部分。此时的优化问题有如下形式：</p>
<div class="math">\begin{equation}\label{eq:AdmmOrigin}minimize \quad f(x)+g(z) \quad w.r.t. \quad Ax+Bz=c\end{equation}</div>
<p><br />
我们将此时的最优值表示为:</p>
<div class="math">\begin{equation}p^{\star}=\text{inf}\{f(x)+g(z)\vert Ax+Bz=c\}\end{equation}</div>
<p><br />
我们对\ref{eq:AdmmOrigin}做一个扩展的拉格朗日变化，可以得到新的目标函数：</p>
<div class="math">\begin{equation}\label{eq:AdmmLag}L_{\rho}(x,z,y)=f(x)+g(z)+y^T(Ax+Bz-c)+(\rho /2)\Vert Ax+Bz-c\Vert _2^2 \end{equation}</div>
<p><br />
ADMM对优化问题\ref{eq:AdmmLag}的迭代更新步骤为：</p>
<div class="math">\begin{equation}\begin{aligned}&amp;x^{k+1}:=\mathop{\argmin}_{x} L_{\rho}(x,z^k,y^k)\\&amp;z^{k+1}:=\mathop{\argmin}_{z} L_{\rho}(x^{k+1},z,y^k)\\&amp; y^{k+1}:=y^k+\rho (Ax^{k+1}+Bz^{k+1}-c)\end{aligned}\end{equation}</div>
<p><br />
在上式中，<span class="math">\(x^k\)</span>的值并没有被用到，所以我们可以把<span class="math">\((z^{k+1},y^{k+1})\)</span>当作<span class="math">\((z^k,y^k)\)</span>的函数。由于<span class="math">\(x,z\)</span>的更新顺序不影响结果，我们可以随意的安排其更新序。</p>
<p>这里我们还提一下ADMM的标量形式，引入一个定义余项<span class="math">\(r=Ax+Bz-c\)</span>，我们可以得到：</p>
<div class="math">\begin{equation}\begin{aligned}y^Tr+(\rho /2) \Vert r\Vert _2^2&amp;=(\rho /2) \Vert r+(1/\rho )y\Vert _2^2 -(1/{2\rho}) \Vert y\Vert _2 ^2\\&amp;=(\rho /2) \Vert r+u\Vert _2^2 -(\rho/2) \Vert u\Vert _2 ^2\\\end{aligned}\end{equation}</div>
<p><br />
其中<span class="math">\(u=(1/\rho)y\)</span>。引入这些定义之后，之前的ADMM更新可以更改为：</p>
<div class="math">\begin{equation}\begin{aligned}&amp;x^{k+1}:=\mathop{\argmin}_{x} f(x)+(\rho /2)\Vert Ax+Bz^k -c+u^k\Vert _2^2\\&amp;z^{k+1}:=\mathop{\argmin}_{z} g(z)+(\rho /2)\Vert Ax^{k+1} +Bz-c +u^k \Vert _2^2\\&amp; u^{k+1}:=u^k+x^{k+1}+Bz^{k+1}-c\end{aligned}\end{equation}</div>
<p><br />
在前两步的更新中，需要处理的都是一个<span class="math">\(primal\)</span>问题。随着迭代的进行，<span class="math">\(u\)</span>将逐渐稳定，而余项<span class="math">\(r\)</span>将逐渐变为0。</p>
<h1 id="9fb890">ADMM收敛性分析</h1>
<p>这里的收敛证明需要一些前提条件，这些前提条件在当前的很多问题中都可以得到满足，这里我们就不去追究其根源。前提条件有二：</p>
<ul>
<li>
<p><span class="math">\(f,g\)</span>的扩展函数<span class="math">\(f: R^n \rightarrow R\cup \{ + \infty\} ,g:R^m \rightarrow \{+\infty\}\)</span>都是一个非空的闭合凸集。这个性质保证了ADMM之中的第一步和第二步更新可以进行。</p>
</li>
<li>
<p>未进行二次增广的拉格朗日乘子形式<span class="math">\(L_0\)</span>拥有一个鞍点，即存在<span class="math">\((x^\star,z^\star,y^\star)\)</span>使得<span class="math">\(L_0(x^\star,z^\star,y)\le L_0(x^\star,z^\star,y^\star)\le L_0(x,z,y^\star)\)</span>。这个性质保证存在一个稳定的最优解。</p>
</li>
</ul>
<p>在上述的两个前提下，当ADMM方法收敛时，下面三个命题都会得到满足：</p>
<ul>
<li>
<p>余项收敛： 随着<span class="math">\(k\rightarrow \infty\)</span>，<span class="math">\(r^k \rightarrow 0\)</span></p>
</li>
<li>
<p>目标值收敛： 随着<span class="math">\(k\rightarrow \infty\)</span>，<span class="math">\(f(x^k)+g(z^k)\rightarrow p^\star\)</span></p>
</li>
<li>
<p>对偶变量收敛： 随着<span class="math">\(k\rightarrow \infty\)</span>，<span class="math">\(y^k\rightarrow y^\star\)</span></p>
</li>
</ul>
<p>需要注意的是，我们当前并不要求<span class="math">\(x^k,z^k\)</span>会得到收敛。</p>
<p>首先我们引入一个定义<span class="math">\(V^k\)</span>：</p>
<div class="math">\begin{equation}\label{eq:Vk}V^k=(1/\rho)\Vert y^k-y^\star\Vert _2^2 +\rho\Vert B(z^k-z^\star)\Vert _2^2\end{equation}</div>
<p>此外，我们还需要利用三个不等式</p>
<div class="math">\begin{equation}\label{eq:prove1}V^{k+1}\le V^k -\rho \Vert r^{k+1}\Vert _2^2 -\rho \Vert B(z^{k+1}-z^k)\Vert _2^2\end{equation}</div>
<div class="math">\begin{equation}\label{eq:prove2}p^{k+1}-p^\star \le -(y^{k+1})^Tr^{k+1}-\rho (B(z^{k+1}-z^k))^T(-r^{k+1}+B(z^{k+1}-z^\star))\end{equation}</div>
<div class="math">\begin{equation}\label{eq:prove3}p^\star -p^{k+1}\le {y^\star}^T r^{k+1}\end{equation}</div>
<p>我们首先对不等式\ref{eq:prove3} 进行证明。考虑到我们的前提条件二<span class="math">\((x^\star,z^\star,y^\star)\)</span>是<span class="math">\(L_0\)</span>的一个鞍点，所以我们有</p>
<div class="math">\begin{equation}p^\star=L_0(x^\star,z^\star,y^\star)\le L_0(x^{k+1},z^{k+1},y^{\star})=f(x^{k+1})+g(z^{k+1})+{y^\star}^T(Ax^{k+1}+Bz^{k+1}-c)\end{equation}</div>
<p><br />
联立<span class="math">\(Ax^{k} +Bz^{k}-c =r^{k}\)</span>以及<span class="math">\(p^{k+1}=f(x^{k+1})+g(z^{k+1})\)</span>,我们就可以得到</p>
<div class="math">\begin{equation}p^\star \le p^{k+1}+{y^\star}^T r^{k+1}\end{equation}</div>
<p>根据定义，<span class="math">\(x^{k+1}\)</span>使<span class="math">\(L_{\rho}(x,z^k,y^k)\)</span>得到最小值，因此</p>
<div class="math">\begin{equation}0\in \partial L_{\rho}(x^{k+1},z^k,y^k)=\partial f(x^{k+1})+A^Ty^k+\rho A^T(Ax^{k+1}+Bz^k-c)\end{equation}</div>
<p><br />
我们将<span class="math">\(y^k=y^{k+1}-\rho r^{k+1}\)</span>带入上式 ，可以得到</p>
<div class="math">\begin{equation}0\in \partial f(x^{k+1})+A^T(y^{k+1}-\rho B(z^{k+1}-z^k))\end{equation}</div>
<p><br />
由此导数可以推出<span class="math">\(x^{k+1}\)</span>使得下面的函数得到最小值：</p>
<div class="math">\begin{equation}f(x)+(y^{k+1}-\rho B(z^{k+1}-z^k))^TAx\end{equation}</div>
<p><br />
所以</p>
<div class="math">\begin{equation}f(x^{k+1})+(y^{k+1}-\rho B(z^{k+1}-z^k))^TAx^{k+1}\le f(x^\star)+(y^{k+1}-\rho B(z^{k+1}-z^k))^TAx^\star\end{equation}</div>
<p><br />
同样的分析过程应用到<span class="math">\(z\)</span>上，可以得到<span class="math">\(z^{k+1}\)</span>使得<span class="math">\(g(z)+y^{(k+1)T}Bz\)</span>得到最小值，因此</p>
<div class="math">\begin{equation}g(z^{k+1})+y^{(k+1)T}Bz^{k+1}\le g(z^\star)+y^{(k+1)T}Bz^\star\end{equation}</div>
<p>将我们新得到的两个不等式累加，并将<span class="math">\(Ax^\star +Bz^\star=c\)</span>带入，即可得到：</p>
<div class="math">\begin{equation}p^{k+1}-p^\star \le -(y^{k+1})^Tr^{k+1}-\rho (B(z^{k+1}-z^k))^T(-r^{k+1}+B(z^{k+1}-z^\star))\end{equation}</div>
<p><br />
因此，不等式\ref{eq:prove3}得证。此时我们再将不等式\ref{eq:prove2}累加并乘以2，可以得到</p>
<div class="math">\begin{equation}\label{eq:prove4}2(y^{k+1}-y^\star)^Tr^{k+1}-2\rho (B(z^{k+1}-z^k))^Tr^{k+1}+2\rho (B(z^{k+1}-z^{k}))^T(B(z^{k+1}-z^\star))\le 0\end{equation}</div>
<p><br />
现在，将<span class="math">\(\rho r^{k+1}=y^{k+1}-y^k=y^{k+1}-y^\star-(y^k-y^\star)\)</span>带入不等式\ref{eq:prove4}的第一项，得到</p>
<div class="math">\begin{equation}(1/\rho)\left(\Vert y^{k+1}-y^\star\Vert _2^2 -\Vert y^k-y^\star\Vert _2^2\right) +2\rho \Vert r^{k+1}\Vert _2^2\end{equation}</div>
<p><br />
同样的，将<span class="math">\(z^{k+1}-z^k=(z^{k+1}-z^\star)+(z^k-z^\star)\)</span>带入到不等式\ref{eq:prove4}的最后一项可以得到</p>
<div class="math">\begin{equation}\rho( \Vert B(z^{k+1}-z^\star)\Vert _2^2-\Vert B(z^{k}-z^\star)\Vert _2^2)+\rho \Vert B(z^{k+1}-z^k)\Vert _2^2\end{equation}</div>
<p><br />
因此，原始的不等式\ref{eq:prove4}可以变换为</p>
<div class="math">\begin{equation}\begin{aligned}&amp;(1/\rho)\left(\Vert y^{k+1}-y^\star\Vert _2^2 -\Vert y^k-y^\star\Vert _2^2\right) +\rho \Vert r^{k+1}\Vert _2^2\\&amp;+\rho \Vert r^{k+1}-B(z^{k+1}-z^k)\Vert _2^2 +\rho( \Vert B(z^{k+1}-z^\star)\Vert _2^2-\Vert B(z^{k}-z^\star)\Vert _2^2) \le 0\end{aligned}\end{equation}</div>
<p><br />
此时我们将原来定义的\ref{eq:Vk}带入，即可得到：</p>
<div class="math">\begin{equation}V^k-V^{k+1}\ge \rho \Vert r^{k+1}-B(z^{k+1}-z^k)\Vert _2^2\end{equation}</div>
<p><br />
为了得到不等式\ref{eq:prove1}，我们只需要证明</p>
<div class="math">\begin{equation}\rho r^{(k+1)T}(B(z^{k+1}-z^k))\le 0\end{equation}</div>
<p><br />
这里我们需要利用<span class="math">\(z^k\)</span>使得<span class="math">\(g(z)+y^{kT}Bz\)</span>取得极小值这个性质。根据定义，<span class="math">\(z^{k+1}\)</span>使得<span class="math">\(L_{\rho}(x^{k+1},z,y^k)\)</span>取到极小值，因此</p>
<div class="math">\begin{equation}\begin{aligned}0&amp;\in \partial g(z^{k+1})+B^Ty^k+\rho B^T(Ax^{k+1}+Bz^{k+1}-c)\\&amp;= \partial g(z^{k+1})+B^Ty^k+\rho B^Tr^{k+1}\\&amp;=\partial g(z^{k+1})+B^Ty^{k+1}\end{aligned}\end{equation}</div>
<p><br />
所以</p>
<div class="math">\begin{equation}\begin{aligned}g(z^{k+1})+y^{(k+1)T}Bz^{k+1}&amp;\le g(z^k)+y^{(k+1)T}Bz^k\\g(z^k)+y^{kT}Bz^k&amp;\le g(z^{k+1})+y^{kT}Bz^{k+1}\end{aligned}\end{equation}</div>
<p><br />
两式相加即可得到</p>
<div class="math">\begin{equation}(y^{k+1}-y^k)(B(z^{k+1}-z^k))\le 0\end{equation}</div>
<p><br />
综上，不等式\ref{eq:prove1}得证。</p>
<p>在不等式\ref{eq:prove1}成立的情况下，由于<span class="math">\(V^k\)</span>是非负的，所以</p>
<div class="math">\begin{equation}\rho \sum_{k=0}^{\infty}{(\Vert r^{k+1}\Vert _2^2+\Vert B(z^{k+1}-z^k)\Vert _2^2)}\le V^0\end{equation}</div>
<p><br />
由于上式是一个无穷和，有上限说明<span class="math">\(r^k\rightarrow 0\)</span>和<span class="math">\(B(z^{k+1}-z^k)\rightarrow 0\)</span>，对于第二部分我们乘以<span class="math">\(\rho A^T\)</span>可以得到<span class="math">\(s^{k+1}=\rho A^TB(z^{k+1}-z^k)\)</span>,这个也是会趋近于0的。在对偶余项收敛的情况下，考虑不等式\ref{eq:prove2}，我们同样可以得出目标值收敛。</p>
<h1 id="5bad24">终止条件分析</h1>
<p>当ADMM方法最终达到最优解时，以下三个条件会得到满足：</p>
<div class="math">\begin{equation}\begin{aligned}&amp;Ax^\star+Bz^\star-c=0\\&amp;0\in \partial f(x^\star)+A^Ty^\star\\&amp;0\in \partial f(z^\star)+B^Ty^\star\end{aligned}\end{equation}</div>
<p><br />
由于<span class="math">\(z^k\)</span>使得<span class="math">\(g(z)+y^{kT}Bz\)</span>取得极小值,所以第三个等式在ADMM迭代中一定成立。只需要考虑前两个</p>
<div class="math">\begin{equation}\begin{aligned}0&amp;\in \partial f(x^{k+1})+A^Ty^k+\rho A^T(Ax^{k+1}+Bz^k-c)\\&amp;=\partial f(x^{k+1})+A^T(y^k+\rho r^{k+1}+\rho B(z^k-z^{k+1}))\\&amp;=\partial f(x^{k+1})+A^Ty^{k+1}+\rho A^TB(z^k-z^{k+1})\end{aligned}\end{equation}</div>
<p><br />
等价的</p>
<div class="math">\begin{equation}\rho A^TB(z^{k+1}-z^k)\in \partial f(x^{k+1})+A^Ty^{k+1}\end{equation}</div>
<p><br />
所以对偶余项<span class="math">\(s^{k}\)</span>趋近于0时，第二个条件越能够被满足。</p>
<p>由于ADMM在高精度求解时收敛会变得很慢，同时大规模机器学习对于精度的要求并没有很高，所以我们一般在对偶残差和初始残差在一定误差内即停止继续迭代。一般的设置为</p>
<div class="math">\begin{equation}\Vert r^k\Vert _2^2\le {\epsilon}^{pri} \quad  \Vert s^k\Vert _2^2\le {\epsilon}^{dual}\end{equation}</div>
<p><br />
我们可以在程序执行之前设定这两个误差界的值，还有另外的一种相对型的误差</p>
<div class="math">\begin{equation}\begin{aligned}{\epsilon}^{pri} &amp;=\sqrt{p} {\epsilon}^{abs}+{\epsilon}^{rel}\text{max}\{\Vert Ax^k\Vert _2，\Vert Bz^k\Vert _2，\Vert c^k\Vert _2\}\\{\epsilon}^{dual} &amp;=\sqrt{n}{\epsilon}^{abs} +{\epsilon}^{rel}\Vert A^Ty^k\Vert _2\end{aligned}\end{equation}</div>
<p><br />
其中的<span class="math">\(p,n\)</span>是输入的维度和样本的个数，byod建议将<span class="math">\({\epsilon}^{rel}\)</span>取值为<span class="math">\(10^{-3}-10^{-4}\)</span>。</p>
<h1 id="7bc220">ADMM计算方法</h1>
<p>在证明了收敛性和给出停止条件之后，现在我们还剩下一个最重要的问题：如何去执行ADMM中的<span class="math">\(x,z\)</span>更新？由于<span class="math">\(x,z\)</span>更新基本是一样的，所以我们只需要考虑如何执行这一步：</p>
<div class="math">\begin{equation}x^{k+1}:=\mathop{\argmin}_{x} f(x)+(\rho /2)\Vert Ax-v\Vert _2^2\end{equation}</div>
<p><br />
其中<span class="math">\(v=-Bz^k +c-u^k\)</span>。</p>
<p>在byod原始的paper中，只讨论了<span class="math">\(f(x)\)</span>的三种形式：</p>
<ul>
<li>
<p>二次函数形式，即<span class="math">\(f(x)=\frac{1}{2}x^TPx+q^Tx+r\)</span>，其中<span class="math">\(P\)</span>为对称半正定。</p>
</li>
<li>
<p>可分的目标函数和约束</p>
</li>
<li>
<p>平滑函数形式</p>
</li>
</ul>
<p>对于二次函数形式，如果<span class="math">\(P+\rho A^TA\)</span>可逆，则我们可以很简单的得到解析解。</p>
<div class="math">\begin{equation}x^{k+1}=(P+\rho A^TA)^{-1}(\rho A^Tv-q)\end{equation}</div>
<p><br />
在计算的时候，需要考虑一些技巧。首先就是缓存<span class="math">\((P+\rho A^TA)^{-1}\)</span>的结果，同时在计算这个逆时，可以采取下面的矩阵逆公式来加速：</p>
<div class="math">\begin{equation}(P+\rho A^TA)^{-1}=P^{-1}-\rho P^{-1}A^T(I+\rho A P^{-1}A^T)^{-1} AP^{-1}\end{equation}</div>
<p>对于没有解析解的情况，我们则需要采取凸优化里面的一些优化方法来得到极小值，这些方法主要包括<span class="math">\(LBFGS\)</span>内点法等。实践中一般会把上一次的值当作这次更新的初始寻找点，然后再利用这些梯度方法来迭代更新。</p>
<p>而对于<span class="math">\(f(x)\)</span>可分块的情况，此时则是大规模分布式计算的最适场景了。如果<span class="math">\(A^TA\)</span>是分块的对角矩阵，那么约束中<span class="math">\(\vert Ax\Vert _2^2\)</span>也是可分的，同时扩展的拉格朗日函数<span class="math">\(L_{\rho}\)</span>也是可分的。特别的，如果<span class="math">\(A=I\)</span>，且<span class="math">\(f(x)=\lambda \Vert x\Vert _1 \, \lambda &gt;0\)</span>。则原始的<span class="math">\(x-update\)</span>过程变为了<code>soft thresholding</code>问题(同时也是优化领域的<code>proximity operator</code>)，即</p>
<div class="math">\begin{equation}x_i^{k+1}=\mathop{\argmin}_{x}(\lambda \Vert x_i\Vert _1 +(\rho /2)\Vert x-v\Vert _2^2)\end{equation}</div>
<p><br />
虽然该目标函数不是处处可导，但是我们可以很容易的得到他的解析解</p>
<div class="math">\begin{equation}x^{+}_i = S_{\lambda/\rho}(v_i),\rightarrow S_k(a) = \left\{\begin{array}{lc}a - k &amp;a &gt;k \\0, &amp; |a|\leq k\\a +k &amp; a &lt; -k \\\end{array}\right.\end{equation}</div>
<p><br />
这里的<span class="math">\(S_k(a)\)</span>也叫做压缩算子(<code>shrinkage operator</code>).</p>
<p>由于这里的每一个变量<span class="math">\(x_i\)</span>都可以单独计算，所以是并行化的理想场所。</p>
<h1 id="022eb3">ADMM问题转化</h1>
<p>现在回到我们最开始的ADMM问题形式：</p>
<div class="math">\begin{equation}\label{eq:AdmmOrigin} minimize \quad f(x)+g(z) \quad w.r.t. \quad Ax+Bz=c \end{equation}</div>
<p><br />
但是，我们一般遇到的是单变量的目标函数，并没有<span class="math">\(z\)</span>的存在。所以，我们需要构造出一个<span class="math">\(z\)</span>出来。下面来谈一下一些常见目标函数的ADMM形式的构建方法。</p>
<p>对于一般的受约束的凸优化问题，我们有如下形式</p>
<div class="math">\begin{equation}minimize \quad f(x) \quad w.r.t. \quad x\in \mathcal{C}\end{equation}</div>
<p><br />
其ADMM形式为</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; f(x) + g(z)\\w.r.t.&amp; x - z = 0 \\\end{array}\Longrightarrow L_{\rho}(x, z, u) = f(x) + g(z) + (\rho/2)\|x - z + u\|^2_2\end{equation}</div>
<p><br />
其中的<span class="math">\(g(x)\)</span>便是<span class="math">\(\mathcal{C}\)</span>的示性函数。同时，该问题的ADMM更新步骤为：</p>
<div class="math">\begin{equation}\begin{split}x^{k+1} &amp; = \arg\min(f(x)+(\rho/2)\|x - z^k + u^k\|^2_2)\\z^{k+1} &amp; = \Pi_{\mathcal{C}}(x^{k+1} + u^k) \\u^{k+1} &amp; = u^k + x^{k+1} - z^{k+1} \\\end{split}\end{equation}</div>
<p><br />
这里的<span class="math">\(\Pi_{\mathcal{C}}\)</span>为向量投影函数。<span class="math">\(x\)</span>的更新则是一个平常的凸函数优化问题，对于可微情况，我们可以直接采用KKT条件来得到极小值。</p>
<p>除了这种普通的凸优化问题以外，还有一些好玩的问题也可以用ADMM来解决。例如找到两个非空包的交集中的一点。其基本算法形式为Neumann交替投影法：</p>
<div class="math">\begin{equation}\begin{split}x^{k + 1} &amp; = \Pi_{\mathcal{C}}(z^k) \\z^{k + 1} &amp; = \Pi_{\mathcal{D}}(x^{k + 1}) \\\end{split}\end{equation}</div>
<p><br />
将这个方法转变为ADMM形式为：</p>
<div class="math">\begin{equation}\begin{split}x^{k + 1} &amp; = \Pi_{\mathcal{C}}(z^k - u^k) \\z^{k + 1} &amp; = \Pi_{\mathcal{D}}(x^{k + 1} + u^k) \\u^{k + 1} &amp; = u^k + x^{k + 1} - z^{k + 1} \\\end{split}\end{equation}</div>
<h1 id="e75465">norm-1 范数问题</h1>
<p>之所以说ADMM适合机器学习和统计学习的优化问题，因为大部分机器学习问题基本都是“损失函数+正则项”形式，这种分法恰好可以套用到ADMM的框架<span class="math">\(f(x)+g(z)\)</span>。因此结合ADMM框架基本可以解决很多已有的问题，以及利用<span class="math">\(\ell_1\)</span>-norm构造的新的优化问题。下面将先介绍非分布式计算的版本，后面会单开一节来介绍如何分布式计算。</p>
<h1 id="ce364f">Least Absolute Deviations</h1>
<p>先从一个简单的问题开始。在稳健估计中，LAD是一个应用很广的模型，相对于直接优化平方和损失<span class="math">\(\Vert Ax-b\Vert _2^2\)</span>,优化绝对损失<span class="math">\(\vert Ax-b\vert _1\)</span>的抗噪性能很好。原始问题可以表示为：</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; \|z\|_1 \\s.t. &amp; Ax - b = z \\\end{array}\end{equation}</div>
<p><br />
在ADMM框架下，往之前的受约束的凸优化问题靠拢，这个问题有简单的迭代算法：</p>
<div class="math">\begin{equation} \text{let} \,\,f(x) = 0, g(z) = \|z\|_1 \Longrightarrow \left \{\begin{split} x^{k + 1} &amp; = (A^TA)^{-1}A^T(b + z^k - u^k) \\z^{k + 1} &amp; = S_{1/\rho}(Ax^{k + 1} - b + u^k) \\u^{k + 1} &amp; = u^k + Ax^{k+1} - z^{k + 1} - b \\\end{split} \right.\end{equation}</div>
<h1 id="f8419e">Huber Fitting</h1>
<p>Huber问题与上面的其实差不多，只是损失函数形式不同，换成了Huber惩罚函数:</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; g^{hub}(z) \\s.t. &amp; Ax - b = z \\\end{array}, \,\, g^{hub}(z) = \left\{\begin{array}{lc}z^2/2, &amp; |z| \leq 1 \\|z| - \frac{1}{2} &amp; |z| &gt; 1 \\\end{array}\right.\end{equation}</div>
<p><br />
因此与LAD除了<span class="math">\(z\)</span>-update不在是proximity operator（或称作软阈值）之外，其余均是相同的</p>
<div class="math">\begin{equation}z^{k + 1} = \frac{\rho}{1 + \rho}(Ax^{k + 1} - b + u^k) + \frac{1}{1 + \rho}S_{1 + 1/\rho}(Ax^{k+1} - b + u^k)\end{equation}</div>
<p><br />
看着像是proximity operator与一个残差的加权。</p>
<p>LAD和Huber fitting这种问题只是一些传统损失不加正则项的ADMM化，注意一定要构造个<span class="math">\(z\)</span>出来即可，<span class="math">\(x\)</span>可以基本不用管，总是需要解的，下面的带有正则项的优化问题，ADMM形式就会更明显。</p>
<h1 id="417b4b">Basis Pursuit</h1>
<p>基追踪法(basic pursuit)是信号处理的一种重要方法。目的是想找到一组稀疏基可以完美恢复信号，换套话说就是为一个线性方程系统找到一个稀疏解。原始形式如下，与lasso有些像：</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; \|x\|_1 \\s.t. &amp; Ax = b \\\end{array}\end{equation}</div>
<p><br />
修改成ADMM形式，注意往之前受约束的凸优化问题的那种形式回套，将<span class="math">\(\ell_1\)</span>看做约束，然后构造带定义域的<span class="math">\(f(x)\)</span>，于是就有解</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; f(x) + \|z\|_1 \\s.t. &amp; x - z = 0 \\\end{array}\,\,\,f(x) = I(\{x \in \mathbf{R}^n| Ax = b\})\Longrightarrow \left \{\begin{split}x^{k + 1} &amp; = \Pi(z^k - u^k) \\z^{k + 1} &amp; = S_{1/\rho}(Ax^{k + 1} +  u^k) \\u^{k + 1} &amp; = u^k + x^{k+1} - z^{k + 1} \\\end{split}\right.\end{equation}</div>
<p><br />
其中<span class="math">\(\Pi(z^k −u^k )\)</span>是向一个线性约束的欧式空间中投影<span class="math">\(x\in R^n ∣Ax=b\)</span>，这也是有直接的显示解的：</p>
<div class="math">\begin{equation}x^{k + 1} = (I - A^T(A^TA)^{-1}A)(z - u^k) + A^T(AA^T)^{-1}b\end{equation}</div>
<p><br />
最近还有一类算法来解决<span class="math">\(ℓ\ell_1\)</span> 问题，被称作Bregman iteration methods，对于基追踪相关问题，加正则项的Bregman iteration就是method of multiplier，而所谓的split Bregman iteration就等同于 ADMM。</p>
<h1 id="5d6d2c">一般化损失函数加norm-1正则化</h1>
<p>一般化损失函数加<span class="math">\(\ell_1\)</span>正则化的形式如：</p>
<div class="math">\begin{equation}\min \,\, l(x) + \lambda\|x\|_1,\end{equation}</div>
<p><br />
这类问题在高维统计开始时便是一个非常重要的问题，而即使到了现在也是一个非常重要的问题，比如group lasso，generalized lasso，高斯图模型，Tensor型图模型，与图相关的<span class="math">\(ℓ\ell_1\)</span> 问题等算法的开发，都可以在此框架上直接应用和实施，这正是ADMM一个优势所在，便于快速实施，也便于可能的大规模分布式部署。</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; l(x) + \lambda \|z\|_1 \\s.t. &amp; x - z = 0 \\\end{array}\Longrightarrow\left \{\begin{split}x^{k + 1} &amp; = \arg\min_x (l(x) + (\rho/2)\|x - z^k + u^k\|_2^2) \\z^{k + 1} &amp; = S_{1/\rho}(x^{k + 1} +  u^k) \\u^{k + 1} &amp; = u^k + x^{k+1} - z^{k + 1} \\\end{split}\right.\end{equation}</div>
<p><br />
特别的，对于<code>lasso</code>问题<span class="math">\(f(x) = \frac{1}{2}|Ax - b|^2_2\)</span>，我们可以得到第一步的解析解：</p>
<div class="math">\begin{equation}x^{k + 1} = (A^TA + \rho I)^{-1}(A^Tb + \rho(z^k - u^k))\end{equation}</div>
<h1 id="46ae36">全局一致问题</h1>
<p>下面两节讲述的两个优化问题，是非常常见的优化问题，也非常重要，是ADMM算法通往并行和分布式计算的一个途径：consensus和sharing，即一致性优化问题与共享优化问题。</p>
<h1 id="ba07db">Global variable consensus optimization</h1>
<p>所谓全局变量一致性优化问题，即目标函数根据数据分解成<span class="math">\(N\)</span>子目标函数（子系统），每个子系统和子数据都可以获得一个参数解<span class="math">\(x^i\)</span> ，但是全局解只有一个<span class="math">\(z\)</span>，于是就可以写成如下优化命题：</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; \sum^N_{i = 1}f_i(x_i), x_i \in \mathbf{R}^n\\s.t. &amp; x_i - z = 0 \\\end{array}\end{equation}</div>
<p><br />
注意，此时<span class="math">\(f_i: \mathbf{R}^n \rightarrow \mathbf{R} \bigcup {+\infty}\)</span>仍是凸函数，而<span class="math">\(x_i\)</span> 并不是对参数空间进行划分，这里是对数据而言，所以<span class="math">\(x_i\)</span> 维度一样$x_i ,z\in R^n $，与之前的问题并不太一样。这种问题其实就是所谓的并行化处理，或分布式处理，希望从多个分块的数据集中获取相同的全局参数解。</p>
<p>在ADMM算法框架下（先返回最初从扩增lagrangian导出的ADMM），这种问题解法相当明确.其问题形式可以表示为：</p>
<div class="math">\begin{equation}\begin{array}{c}L_{\rho}(x_1, \ldots, x_N, z, y) = \sum^N_{i=1}(f_i(x_i) + y^T_i(x_i - z) + (\rho/2)\|x_i - z\|^2_2) \\s.t. \mathcal{C} = \{(x_1, \ldots, x_N)|x_1 = \ldots = x_N\} \\\end{array} \end{equation}</div>
<p><br />
其ADMM更新方法见下：</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_x (f_i(x_i) + (y^k_i)^T(x_i - z^k) +  (\rho/2)\|x_i - z\|^2_2)) \\z^{k+1} &amp; = \frac{1}{N}\sum^N_{i=1}(x_i^{k+1} + (\frac{1}{\rho}y^k_i)) \\y_i^{k+1} &amp; = y_i^k + \rho(x_i^{k+1} - z^{k+1}) \\\end{split}\end{equation}</div>
<p><br />
对<span class="math">\(y\)</span>-update和<span class="math">\(z\)</span>-update的<span class="math">\(y^{k+1}_ i\)</span> 和<span class="math">\(z^{k+1}_ i\)</span> 分别求个平均，易得<span class="math">\(\bar{y}^{k+1} =0\)</span>，于是可以知道<span class="math">\(z\)</span>-update步其实可以简化为<span class="math">\(z^{ k+1} =\bar{x}^{k+1}\)</span> ，于是上述ADMM其实可以进一步化简为如下形式：</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_x (f_i(x_i) + (y^k_i)^T(x_i - \bar{x}^k) + (\rho/2)\|x_i - \bar{x}^k\|^2_2)) \\y_i^{k+1} &amp; = y_i^k + \rho(x_i^{k+1} - \bar{x}^{k+1}) \\\end{split}\end{equation}</div>
<p><br />
这种迭代算法写出来了，并行化那么就是轻而易举了，各个子数据分别并行求最小化，然后将各个子数据的解汇集起来求均值，整体更新对偶变量<span class="math">\(y^k\)</span> ，然后再继续回带求最小值至收敛。当然也可以分布式部署（hadoop化），但是说起来容易，真正工程实施起来又是另外一回事，各个子节点机器间的通信更新是一个需要细细揣摩的问题。</p>
<p>另外，对于全局一致性优化，也需要给出相应的终止迭代准则，与一般的ADMM类似，看primal和dual的residuals即可</p>
<div class="math">\begin{equation}\|r^k\|_2^2 = \sum^N_{i = 1}\|x^k_i - \bar{x}^k\|_2^2, \quad \|s^k\|_2^2 = N\rho\|\bar{x}^k_i - \bar{x}^{k-1}\|_2^2\end{equation}</div>
<h1 id="780082">带正则项的全局一致性问题</h1>
<p>下面就是要将之前所谈到的经典的机器学习算法并行化起来。想法很简单，就是对全局变量加上正则项即可，</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; \sum^N_{i = 1}f_i(x_i) + g(z), x_i \in \mathbf{R}^n\\s.t. &amp; x_i - z = 0 \\\end{array}\end{equation}</div>
<p><br />
因此ADMM算法只需要改变下<span class="math">\(z\)</span>-update步即可</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_{x+i} (f_i(x_i) + (y^k_i)^T(x_i - z^k) (\rho/2)\|x_i - z\|^2_2)) \\z^{k+1} &amp; = \arg\min_z (g(z) + \sum^N_{i=1}(-(y^k_i)^Tz + (\rho/2)\|x^{k+1}_i - z\|_2^2)) \\y_i^{k+1} &amp; = y_i^k + \rho(x_i^{k+1} - z^{k+1})\\\end{split}\end{equation}</div>
<p><br />
同样的，我们仍对<span class="math">\(z\)</span>做一个平均处理，于是就有</p>
<div class="math">\begin{equation}z^{k+1} = \arg\min_z(g(z) + (N\rho/2)\|z - \bar{x}^{k+1} - (1/\rho)\bar{y}^k\|^2_2)\end{equation}</div>
<p> <br />
上述形式都取得是最原始的ADMM形式，简化处理，写成scaled形式即有</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_x (f_i(x_i) + (\rho/2)\|x_i - z^k + u_i^k\|^2_2)) \\z^{k+1} &amp; = \arg\min_z (g(z) + (N\rho/2)\|z - x^{k+1}_i - \bar{u}^k\|^2_2) \\u_i^{k+1} &amp; = u_i^k + x_i^{k+1} - z^{k+1} \\\end{split}\end{equation}</div>
<p><br />
这样对于后续处理问题就清晰明了多了。可以看到如果<span class="math">\(g(z)=\lambda \|z\|_1\)</span> ，即lasso问题，那么<span class="math">\(z\)</span>-update步就用软阈值operator即可。因此，对于大规模数据，要想用lasso等算法，只需要对数据做切块（切块也最好切均匀点），纳入到全局变量一致性的ADMM框架中，即可并行化处理。下面给出一些实例。</p>
<h1 id="57a4a9">数据切割</h1>
<p>在经典的统计估计中，我们处理的多半是大样本低维度的数据，现在则多是是大样本高维度的数据。对于经典的大样本低维度数据，如果机器不够好，那么就抽样部分数据亦可以实现较好估计。不过如果没有很好的信息，就是想要对大样本进行处理，那么切割数据，并行计算是一个好的选择。现在的社交网络、网络日志、无线感应网络等都可以这么实施。下面的具体模型都在受约束的凸优化问题中以及<span class="math">\(\ell_1\)</span>-norm问题中提过，此处只不过切割数据，做成分布式模型，思想很简单，与带正则项的global consensus问题一样的处理。经典问题lasso、sparse logistic lasso、SVM都可以纳入如下框架处理。</p>
<p>有观测阵<span class="math">\(A\in R^{m×n}\)</span> 和响应值<span class="math">\(b\in R^m\)</span> ，可以对应切分，即对矩阵<span class="math">\(A\)</span>和向量<span class="math">\(b\)</span>横着切，</p>
<div class="math">\begin{equation}A = \begin{pmatrix}A_1\\\vdots\\A_N \\\end{pmatrix}\quad b = \begin{pmatrix}b_1 \\\vdots\\b_N \\\end{pmatrix}\end{equation}</div>
<p> <br />
于是原来带正则项的优化问题就可以按照数据分解到多个子系统上去分别优化，然后汇集起来，形成一个global consensus问题。</p>
<div class="math">\begin{equation}\begin{array}{lr}\min &amp; \sum^N_{i=1}l_i(A_ix_i - b_i) + r(z) \\s.t. &amp; x_i - z = 0, i = 1, \ldots, N \quad x_i, z \in \mathbf{R}^n \\\end{array}\end{equation}</div>
<p><br />
结合受约束的凸优化问题时所给出来的具体的ADMM算法解的形式，下面直接给出这些问题的ADMM迭代算法公式。</p>
<p>对于lasso，其更新公式为</p>
<div class="math">\begin{equation}\begin{split}x_i^{k + 1} &amp; = (A_i^TA_i + \rho I)^{-1}(A_i^Tb_i + \rho (z^k - u_i^k)) \\z^{k + 1} &amp; = S_{1/\rho N}(\bar{x}^{k + 1} - b + \bar{u}^k) \\u_i^{k + 1} &amp; = u_i^k + x_i^{k+1} - z^{k + 1} \\\end{split}\end{equation}</div>
<p><br />
如果切割的数据量小于维数<span class="math">\(m_i &lt;n\)</span>，那么求解时分解小的矩阵<span class="math">\(A_i A^T_i +\rho I\)</span>即可；其他求逆采用矩阵加速技巧即可。</p>
<p>对于Sparse Logistic Regression,其更新公式如下：</p>
<div class="math">\begin{equation}\begin{split}x_i^{k + 1} &amp; =  \arg\min_{x_i}(l_i(A_ix_i - b_i) + (\rho/2)\|x_i - z^k + u_i^k\|^2_2\\z^{k + 1} &amp; = S_{1/\rho N}(\bar{x}^{k + 1} - \bar{b} + \bar{u}^k) \\u_i^{k + 1} &amp; = u_i^k + x_i^{k+1} - z^{k + 1} \\\end{split}\end{equation}</div>
<p><br />
在<span class="math">\(x\)</span>-update步是需要用一些有效的算法来解决<span class="math">\(\ell_2\)</span> 正则的logistic回归，比如L-BFGS，其他的优化算法应该问题不大吧。</p>
<p>对于SVM，注意分类问题和回归问题的损失函数不同，一般都是用<span class="math">\(l(sign(t)y)\)</span>形式来寻求最优的分类权重使得分类正确。SVM使用Hinge Loss：<span class="math">\(\ell(y)=max(0,1−t.y)\)</span>，即将预测类别与实际分类符号相反的损失给凸显出来。分布式的ADMM形式为</p>
<div class="math">\begin{equation}\begin{split}x_i^{k + 1} &amp; =  \arg\min_{x_i}(\mathbf{1}^T(A_ix_i + 1)_{+} + (\rho/2)\|x_i - z^k + u_i^k\|^2_2\\z^{k + 1} &amp; = \frac{\rho}{(1/\lambda) + N\rho}(\bar{x}^{k+1} + \bar{u}^k) \\u_i^{k + 1} &amp; = u_i^k + x_i^{k+1} - z^{k + 1} \\\end{split}\end{equation}</div>
<h1 id="68467d">一般形式的一致性优化问题</h1>
<p>所谓的一般形式的一致性问题，就是切割参数到各子系统，但各子系统目标函数参数维度不同，可能部分重合。上述全局一致性优化问题中，我们可以看到，所做的处理不过是对数据分块，然后并行化处理。但是更一般的优化问题是，参数空间也是分块的，即每个子目标函数<span class="math">\(f_i (x_i )\)</span>的参数维度不同<span class="math">\(x_i ,\in R n_i\)</span>  ，我们称之为局部变量。而局部变量所对应的的也将不再是全局变量<span class="math">\(z\)</span>，而是全局变量中的一部分<span class="math">\(z_g\)</span> ，并且不是像之前的顺序对应，而可能是随便对应到<span class="math">\(z\)</span>的某个位置.可令<span class="math">\(g=\mathcal{G}(i,.)\)</span>，即将<span class="math">\(x_i\)</span> 映射到<span class="math">\(z\)</span>的某部位</p>
<div class="math">\begin{equation}(x_i)_j = z_{\mathcal{G}(i, j)} = \hat{z}_i \end{equation}</div>
<p><br />
如果对所有<span class="math">\(i\)</span>有<span class="math">\(\mathcal{G}(i,j)=j\)</span>，那么<span class="math">\(x_i\)</span> 与<span class="math">\(z\)</span>就是顺序映射，也就是全局一致性优化问题，否则就不是。结合下图就比较好理解</p>
<p><img alt="Consensus" src="http://spiritsaway.info/ai/image/ADMM/ADMM_consensus.png" /></p>
<p>虽然如果用其他方法来做感觉会复杂，但是纳入到上述ADMM框架，其实只不过是全局一致性优化问题的一个局部化变形，不过此时不是对数据进行分块，是对参数空间进行分块.新的优化问题可以表示为</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; \sum^N_{i = 1}f_i(x_i) + g(z), x_i \in \mathbf{R}^{n_i}\\s.t. &amp; x_i - \hat{z}_i = 0, i = 1, \ldots N \\\end{array}\end{equation}</div>
<p><br />
而新的更新策略则为</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_x (f_i(x_i) + (y^k_i)^Tx_i (\rho/2)\|x_i - \hat{z}_i^k\|^2_2)) \\z^{k+1} &amp; = \arg\min_z (\sum^N_{i=1}(-(y^k_i)^T\hat{z}_i + (\rho/2)\|x^{k+1}_i - \hat{z}_i\|_2^2))) \\y_i^{k+1} &amp; = y_i^k + \rho(x_i^{k+1} - \hat{z}_i^{k+1}) \\\end{split}\end{equation}</div>
<h1 id="483c09">共享问题</h1>
<p>与之前的全局变量一致性优化问题类似，共享问题也是一个非常一般而且常见的问题。他的形式如下：</p>
<div class="math">\begin{equation}\min \,\, \sum^N_{i=1}f_i(x_i) + g(\sum^N_{i=1}x_i)\end{equation}</div>
<p> <br />
这里的第一部分局部损失<span class="math">\(f_i (x_i )\)</span>与全局一致性优化是一样的，即所有的<span class="math">\(x_i \in R^n ,i=1,\cdots,N\)</span>同维度，而对于一个共享的目标函数<span class="math">\(g\)</span>则是新加入的。在实际中，我们常常需要优化每个子数据集上的损失函数，同时还要加上全局数据所带来的损失；或者需要优化每个子系统的部分变量，同时还要优化整个变量。共享问题是一个非常重要而灵活的问题，它也可以纳入到ADMM框架中，形式如下：</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; \sum^N_{i=1}f_i(x_i) + g(\sum^N_{i=1}z_i) \\s.t. &amp; x_i - z_i = 0, z_i \in \mathbf{R}^n, i = 1, \ldots, N, \\\end{array}\end{equation}</div>
<p><br />
其更新形式如下：</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_{x_i} (f_i(x_i) + (\rho/2)\|x_i - z_i^k + u_i^k\|^2_2)) \\z^{k+1} &amp; = \arg\min_z (g(\sum^N_{i = 1}z_i) + \rho/2\sum^N_{i = 1}\|z_i - x^{k+1}_i - u^k_i\|^2_2) \\u_i^{k+1} &amp; = u_i^k + x_i^{k+1} - z_i^{k+1} \\\end{split}\end{equation}</div>
<p><br />
上述形式当然还不够简洁，需要进一步化简。因为<span class="math">\(x\)</span>-update可以不用担心，分机并行处理优化求解即可，而对于<span class="math">\(z\)</span>-update这里面需要对<span class="math">\(N\)</span>个变量求解，想加快速度，就减少变量个数。于是想办法通过和之前那种平均方式一样来简化形式解.</p>
<p>对于<span class="math">\(z\)</span>-update步，令<span class="math">\(a_i =u^k_i +x^{k+1}_i\)</span> ，于是<span class="math">\(z\)</span>-update步优化问题转化为</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; g(N\bar{z}) + (\rho/2)\sum^N_{i=1}\|z_i - a_i\|^2_2 \\s.t. &amp; \bar{z} = \frac{1}{N}\sum^N_{i=1}z_i \\\end{array}\end{equation}</div>
<p><br />
当<span class="math">\(\bar{z}\)</span> 固定时，那么后面的最优解（类似回归）为<span class="math">\(z_i =a_i +\bar{z} −\bar{a }\)</span>，带入上式后于是后续优化就开始整体更新（均值化）</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_{x_i} (f_i(x_i) + (\rho/2)\|x_i - x_i^k + \bar{x}^k - \bar{z}^k + u^k\|^2_2)) \\z^{k+1} &amp; = \arg\min_z (g(N\bar{z}) + N\rho/2\|\bar{z} - \bar{x}^{k+1} - u^k\|^2_2) \\u^{k+1} &amp; = u_i^k + \bar{x}^{k+1} - \bar{z}^{k+1} \\\end{split}\end{equation}</div>
<p><br />
另外，有证明如果强对偶性存在，那么global consensus问题与sharing问题是可以相互转化的，可以同时达到最优，两者存在着很紧密的对偶关系。</p>
<p>本节开头提过，sharing问题用来切分数据做并行化，也可以切分参数空间做并行化。这对于高维、超高维问题是非常有好处的。因为高维统计中，大样本是一方面问题，而高维度才是重中之重，如果能切分特征到低纬度中去求解，然后在合并起来，那么这将是一个很美妙的事情。上面利用regularized global consensus问题解决了切分大样本数据的并行化问题，下面利用sharing思想解决常见的高维数据并行化问题</p>
<p>同样假设面对还是一个观测阵<span class="math">\(A\in R^{m×n}\)</span>和响应观测<span class="math">\(b\in R^n\)</span> ，此时有<span class="math">\(n\gg m\)</span>，那么要么就降维处理，要么就切分维度去处理，或者对于超高维矩阵，切分维度后再降维。此时<span class="math">\(A\)</span>矩阵就不是像之前横着切分，而是竖着切分，这样对应着参数空间的切分：</p>
<div class="math">\begin{equation}A = [A_1, \ldots, A_N], A_i \in \mathbf{R}^{m \times n_i}, x = (x_1, \ldots, x_N), x\in \mathbf{R}^{n_i}, \rightarrow Ax = \sum^N_{i = 1}A_ix_i\end{equation}</div>
<p> <br />
于是正则项也可以切分为<span class="math">\(r(x) = \sum^N_{i = 1}r_i(x_i)\)</span>，那么从最初的<span class="math">\(\min \,\, l(Ax - b) + r(x)\)</span>形式变成了：</p>
<div class="math">\begin{equation}\min \,\, l(\sum^N_{i = 1}A_ix_i - b) + \sum^N_{i = 1}r_i(x_i)\end{equation}</div>
<p><br />
 个与sharing问题非常接近了，做点变化那就是sharing问题了</p>
<div class="math">\begin{equation}\begin{array}{lc}\min &amp; l(\sum^N_{i=1}z_i - b) + \sum^N_{i=1}r_i(x_i) \\s.t. &amp; A_ix_i - z_i = 0, i = 1,\ldots, N \\\end{array}\end{equation}</div>
<p><br />
其对应的更新形式如下：</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_{x_i} (r_i(x_i) + (\rho/2)\|A_ix_i - A_ix_i^k + \overline{Ax}^k - \bar{z}^k + u^k\|^2_2)) \\z^{k+1} &amp; = \arg\min_z (l(N\bar{z} - b) + N\rho/2\|\bar{z} - \overline{Ax}^{k+1} - u^k\|^2_2) \\u^{k+1} &amp; = u_i^k + \overline{Ax}^{k+1} - \bar{z}^{k+1} \\\end{split}\end{equation}</div>
<p><br />
与之前的global consensus问题相比，ADMM框架<span class="math">\(x\)</span>-update与<span class="math">\(z\)</span>-update似乎是反过来了。于是将此形式直接套到Lasso等高维问题即有很具体的形式解了</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_{x_i} (\lambda\|x_i\|_1+ (\rho/2)\|A_ix_i - A_ix_i^k + \overline{Ax}^k - \bar{z}^k + u^k\|^2_2)) \\\bar{z}^{k+1} &amp; = \frac{1}{N + \rho}(b + \rho \overline{Ax}^{k+1} + \rho u^k) \\u^{k+1} &amp; = u^k + \overline{Ax}^{k+1} - \bar{z}^{k+1} \\\end{split}\end{equation}</div>
<p><br />
当<span class="math">\(|A^T_i(A_ix^k_i + \bar{z}^k - \overline{Ax}^k - u^k)|_2 \leq \lambda/\rho\)</span>时<span class="math">\(x^{k+1}_i = 0\)</span>，所以这样加快了x -update速度,不过这个对串行更有效，对并行起来也没有多大用..</p>
<p>Sparse Logstic Regression 也与lasso区别不大，只是<span class="math">\(z\)</span>-update的损失函数不同，其余相同于是</p>
<div class="math">\begin{equation}\bar{z}^{k+1}  = \arg\min_{\bar{z}} (l(N\bar{z})+ (\rho/2)\|\bar{z} - \overline{Ax}^{k+1} - u^k\|^2_2)\end{equation}</div>
<p>SVM与之前的global consensus时候优化顺序反了过来，与logistic rgression只是在<span class="math">\(z\)</span>-update步不同（损失函数不同）：</p>
<div class="math">\begin{equation}\begin{split}x_i^{k+1} &amp; = \arg\min_{x_i} (\lambda\|x_i\|_2^2+ (\rho/2)\|A_ix_i - A_ix_i^k + \overline{Ax}^k - \bar{z}^k + u^k\|^2_2)) \\\bar{z}^{k+1} &amp; = \arg\min_{\bar{z}}(\mathbf{1}^T(N\bar{z}+\mathbf{1})_{+} + (\rho/2)\|\bar{z} - \overline{Ax}^{k+1} - u^{k+1}\|) \\u^{k+1} &amp; = u^k + \overline{Ax}^{k+1} - \bar{z}^{k+1} \\\end{split}\end{equation}</div>
<p><br />
<span class="math">\(z\)</span>-update解析解可以写成软阈值算子</p>
<div class="math">\begin{equation}(\bar{z}^{k+1})_i = \left\{\begin{array}{ll}v_i - N/\rho, &amp; v_i &gt; -1/N + N/\rho \\-1/N, &amp; v_i \in [-1/N, -1/N + N/\rho] \\v_i, &amp; v_i &lt; -1/N \\\end{array}\right.v_i = (\overline{Ax}^{k+1} + \bar{u}^k)_i\end{equation}</div>
<h1 id="ea6f3b">参考链接</h1>
<ul>
<li>
<p>byod在2011年发表的关于ADMM的综述文章：<a href="http://web.stanford.edu/~boyd/papers/admm_distr_stats.html">http://web.stanford.edu/~boyd/papers/admm_distr_stats.html</a></p>
</li>
<li>
<p>高涛对于上面这篇文章的部分翻译和一些归纳总结（本文后期基本抄自此翻译）<a href="http://joegaotao.github.io/cn/2014/02/admm/">http://joegaotao.github.io/cn/2014/02/admm/</a></p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
    <div id="content-comments">
<div id="disqus_thread"></div>
<script type="text/javascript">var disqus_shortname="SpiritsAway";(function(){var a=document.createElement("script");a.type="text/javascript";a.async=true;a.src="//"+disqus_shortname+".disqus.com/embed.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)})();</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

</div>
<div class="niu2-right-container col-md-2">
    <div id="niu2-sidebar-meta" class="niu2-sidebar">
        <div class="niu2-sidebar-label"><i class="fa fa-calendar"></i>发布时间:</div>
        <div class="niu2-sidebar-value">2015-05-07</div>
        <div class="niu2-sidebar-label"><i class="fa fa-open-folder"></i>分类:</div>
        <div class="niu2-sidebar-value"><a href="http://spiritsaway.info/ai/index.html">AI</a></div>
        <div class="niu2-sidebar-label"><i class="fa fa-tag"></i>标签:</div>
 
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="http://spiritsaway.info/tag/admm.html">ADMM</a><sup>1</sup></div>
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="http://spiritsaway.info/tag/math.html">Math</a><sup>5</sup></div>
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="http://spiritsaway.info/tag/ai.html">AI</a><sup>5</sup></div>
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="http://spiritsaway.info/tag/converge.html">Converge</a><sup>5</sup></div>
    </div>

    <div id="niu2-sidebar-toc" class="niu2-sidebar" data-status="closed">
        <div class="niu2-sidebar-label">
            <i id="niu2-sidebar-toc-ctrl" class="fa fa-plus"></i>目录
        </div>
        <a href="#content-heading">ADMM</a>
        <ol id="niu2-sidebar-toc-list">
            <li><a href='#844962'>扩展的拉格朗日乘子方法</a></li><li><a href='#d20776'>ADMM介绍</a></li><li><a href='#9fb890'>ADMM收敛性分析</a></li><li><a href='#5bad24'>终止条件分析</a></li><li><a href='#7bc220'>ADMM计算方法</a></li><li><a href='#022eb3'>ADMM问题转化</a></li><li><a href='#e75465'>norm-1 范数问题</a></li><li><a href='#ce364f'>Least Absolute Deviations</a></li><li><a href='#f8419e'>Huber Fitting</a></li><li><a href='#417b4b'>Basis Pursuit</a></li><li><a href='#5d6d2c'>一般化损失函数加norm-1正则化</a></li><li><a href='#46ae36'>全局一致问题</a></li><li><a href='#ba07db'>Global variable consensus optimization</a></li><li><a href='#780082'>带正则项的全局一致性问题</a></li><li><a href='#57a4a9'>数据切割</a></li><li><a href='#68467d'>一般形式的一致性优化问题</a></li><li><a href='#483c09'>共享问题</a></li><li><a href='#ea6f3b'>参考链接</a></li>
            <li><a href="#content-comments">评论</a></li>
        </ol>
    </div>
</div>
    </div>

    <div id="body-footer" class="col-md-6 col-md-offset-3">
<div class="niu2-footer">
    <hr/>
	<p>
		Powered by <a href="https://github.com/getpelican/pelican">Pelican</a>, 
        theme built with <a href="http://getbootstrap.com">Bootstrap3</a> 
		modified by <a href="https://github.com/FinalTheory/pelican-theme">FinalTheory</a>, 
		icons by <a href="http://fortawesome.github.io/Font-Awesome">Font Awesome</a>.
	</p>
	<p>
		COPYRIGHT ©
				2015
		<a class="niu2-footer-link" href="http://spiritsaway.info">Huiliang Huang</a>
    </p>
</div>    </div>
    
    <div id="niu2-pygments" data-theme="github"></div>

    <script type="text/javascript" src="http://spiritsaway.info/theme/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="http://spiritsaway.info/theme/js/niu2.js"></script>
    <script type="text/javascript" src="http://spiritsaway.info/theme/js/bootstrap.min.js"></script>
    <script type="text/javascript">onContentLoaded();</script>
  </body>
</html>