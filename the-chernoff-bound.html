<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Chernoff Bound - spirits away</title>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.png">
    <link rel="stylesheet" href="./theme/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="./theme/font-icons/style.min.css" type="text/css" />
    <link rel="stylesheet" href="./theme/css/niu2.min.css" type="text/css" />

    <link rel="canonical" href="./the-chernoff-bound.html" />
    
    <script type="text/javascript">window.onload=function(){};</script>
    <!--[if lt IE 9]>
        <script src="./theme/js/html5shiv.js"></script>
        <script src="./theme/js/respond.min.js"></script>
    <![endif]-->
  </head>
  <body> 
    <div id="body-header">
<div class="navbar navbar-inverse navbar-fixed-top">
<div class="col-md-12">
    <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href=".">
            <i class="icon-home"></i>spirits away
        </a>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
        <ul class="nav navbar-nav">
            <li><a href="/pages/about.html" title="about">
                <i class="icon-anchor"></i>关于</a>
            </li>
            <li><a href="/archives.html" title="archives">
                <i class="icon-archive"></i>存档</a>
            </li>
            <li><a href="/tags.html" title="tags">
                <i class="icon-tag"></i>标签</a>
            </li>
        <!-- category dropdown list -->
        <!--  self defined dropdown list -->
        </ul>
        
        <!-- right nav bar -->
        <ul class="nav navbar-nav navbar-right">
        <!-- google custom search -->
       </ul>
    </nav>
</div>
</div>    </div>
    
    <div id="body-content">
<div class="col-md-8 col-md-offset-2">
    <h1 id="content-heading">The Chernoff Bound</h1>
</div>
<div id="niu2-left-container" class="col-md-6 col-md-offset-2 with-right-border">
    <div id="niu2-main-content">
        <p>本文主要介绍一下chernoff bound的证明，以及一些在统计学习理论中的涉及到的一些不等式,首先来几个简单的。</p>
<p>Jensen 不等X为随机变量，<span class="math">\(f:X\rightarrow\mathbb{R}\)</span>为凸函数,则下面的不等式成立</p>
<div class="math">\begin{equation}f(\mathbb{E}[X])\leq \mathbb{E}[f(x)] \end{equation}</div>
<p>
这个就是凸函数的性质，这里并不给出证明了，想知道为什么的看一下《凸优化》的前几章就行了。</p>
<p>接下来是Markov不等式，这里终于牵涉到概率了。假设X为非负的随机变量，且<span class="math">\(t&gt;0\)</span>，则下式成立。</p>
<div class="math">\begin{equation}P(X&gt;t)\leq\frac{\mathbb{E}[X]}{t} \end{equation}</div>
<p>
这里的证明也很简单</p>
<div class="math">\begin{equation}\begin{array}{rl} \mathbb{E}[X]&amp;=\int_{0}^{+\infty}xp(x)dx=\int_{0}^{t}xp(x)dx+\int_{t}^{+\infty}xp(x)dx\\ &amp;\geq \int_{t}^{+\infty}xp(x)dx \geq t\int_{t}^{+\infty}p(x)dx=tP(X&gt;t) \end{array} \end{equation}</div>
<p>现在重点来了，我们隆重介绍一下chernoff bound。首先来看一下最简单形式的chernoff bound。</p>
<p>假设<span class="math">\(X\)</span>为随机变量，则下面的不等式成立</p>
<div class="math">\begin{equation}P(X&gt;\epsilon)\leq\underset{t\geq 0}{\inf}\exp(-t\epsilon)\mathbb{E}[\exp(tX)] \end{equation}</div>
<p>
这个不等式的证明也比较简单，直接利用之前的markov不等式即可。</p>
<div class="math">\begin{equation}\begin{array}{rl}P(X&gt;\epsilon)&amp;=P\left(\exp(X)&gt;\exp(\epsilon)\right)\\ &amp;=\underbrace{P\left(\exp(tX)&gt;\exp(t\epsilon)\right)\leq\exp(-t\epsilon)\mathbb{E}[\exp(tX)]}_{Markov's \; inequality}\end{array} \end{equation}</div>
<p>
粗略一看，这个式子好像没有什么用。的确，在任何情况下都成立基本没啥用，我们需要在不同的情况下推广他的扩展形式</p>
<h1 id="7d0e78">在期望已知的情况下Chernoff Bound的变体</h1>
<p>假设随机变量<span class="math">\(X\in[a,b]\)</span>,<span class="math">\(\mathbb{E}[X]=0\)</span>，则下面的不等式成立。</p>
<div class="math">\begin{equation}\mathbb{E}[\exp(tX)]\leq\exp\left(\frac{t^2(b-a)^2}{8}\right) \end{equation}</div>
<p>
这个不等式，粗看起来与之前提到的Chernoff Bound有点相似，但是只是在左边相似而已。事实上不等式左边的基本毫无意义，怎么得到右边的公式才是最重要的。在得到右边的过程中，我们需要利用<span class="math">\(f(x)=\exp(tx)\)</span>为凸函数的性质。</p>
<p>证明如下：在<span class="math">\(a\neq 0 ,b\neq 0\)</span>时，我们可以把<span class="math">\(X\)</span>表示为<span class="math">\(a,b\)</span>的线性组合</p>
<div class="math">\begin{equation}X=\alpha b+(1-\alpha)a\end{equation}</div>
<p>
其中<span class="math">\(\alpha=(X-a)/(b-a)\in[0,1]\)</span>。接下来我们利用凸函数的性质，即Jensen不等式</p>
<div class="math">\begin{equation}f(X)=f(\alpha x+(1-\alpha)b)\leq\alpha f(x)+(1-\alpha)f(b) \end{equation}</div>
<p>
将<span class="math">\(X=\alpha b+(1-\alpha)a\)</span>带入到上面的不等式中，我们得到</p>
<div class="math">\begin{equation}\exp(tX)\leq\frac{X-a}{b-a}\exp(tb)+\frac{b-X}{b-a}\exp(ta)\end{equation}</div>
<p>
对该不等式求期望，并考虑到我们之前的大前提<span class="math">\(\mathbb{E}[X]=0\)</span>，我们得到下面的不等式</p>
<div class="math">\begin{equation}\mathbb{E}[\exp(tX)]\leq-\frac{a}{b-a}\exp(tb)+\frac{b}{b-a}\exp(ta)=\exp(g(u)) \end{equation}</div>
<p>
其中<span class="math">\(u=t(b-a),g(u)=-\theta u+\log (1-\theta+\theta \exp(u)),\theta=-a/(b-a)\in(0,1)\)</span>。看上去我们把东西弄得越来越复杂，接下来是见证奇迹的时刻。根据Taylor展开公式，存在<span class="math">\(v\in (0,u)\)</span>使得下面的等式成立：</p>
<div class="math">\begin{equation}g(u)=g(0)+ug'(0)+\frac{u^2}{2}g''(v) \end{equation}</div>
<p>
下面来对<span class="math">\(g(u)\)</span>求一阶导数和二阶导数</p>
<div class="math">\begin{equation}g'(0)=-\theta+\frac{\theta\exp(u)}{1-\theta+\theta\exp(u)}\vert_{u=0}=0\end{equation}</div>
<p>\begin{equation}\end{equation}
二阶导数的放缩利用了<span class="math">\(ab\leq (a+b)^2/4\)</span>这个最简单的不等式。接下来我们将上面得到的结果都带入原来的taylor展开式中</p>
<div class="math">\begin{equation}g(u)=\frac{u^2}{2}g''(v)\leq\frac{u^2}{8}=\frac{t^2(b-a)^2}{8} \end{equation}</div>
<p>
由此，经历了千辛万苦，我们终于证明了</p>
<div class="math">\begin{equation}\mathbb{E}[\exp(tX)]\leq\exp\left(\frac{t^2(b-a)^2}{8}\right) \end{equation}</div>
<h1 id="8fe964">在bernoulli分布中的Chernoff Bound</h1>
<p>之前得到的Chernoff界是绝对差形式，这里的Chernoff界则是相对差形式。假设<span class="math">\(X_1,X_2,\cdots,X_n\)</span>为符合bernoulli分布的各自独立的随机变量，<span class="math">\(X=\sum {X_i}\)</span>，且<span class="math">\(\mu \geq E[X]\)</span>。则对于任意的<span class="math">\(\delta &gt;0\)</span>，下面的不等式都成立</p>
<div class="math">\begin{equation}Pr[X&gt;(1+\delta)\mu]&lt;\left[ \frac{e^{\delta}}{{(1+\delta)}^{(1+\delta)}}\right]^{\mu}\end{equation}</div>
<p>
这个不等式跟之前的不等式相比，简直丑爆了，而且右边更加复杂。但是证明的第一步还是相同的，还是markov不等式。</p>
<div class="math">\begin{equation}Pr[X&gt;(1+\delta)\mu]=Pr\left[e^{tX}&gt;e^{t(1+\delta)\mu}\right]\leq e^{-t(1+\delta)\mu}\mathbb{E}[e^{tX}]\end{equation}</div>
<p>
又由于<span class="math">\(X=\sum {X_i}\)</span>，所以</p>
<div class="math">\begin{equation}\mathbb{E}[e^{tX}]=\mathbb{E}\left[e^{t\sum {X_i}}\right]=\mathbb{E}\left[\prod{e^{tX_i}}\right]\end{equation}</div>
<p>
这里利用了<span class="math">\(\mathbb{E}[YZ]=\mathbb{E}[Y]*\mathbb{E}[Z]\)</span>这个独立变量的积的期望公式。又由于<span class="math">\(X_i\)</span>是符合伯努利分布的随机变量，假设其参数为<span class="math">\(Pr[X_i=1]=p_i\)</span>，则我们有</p>
<div class="math">\begin{equation}\mathbb{E}\left[e^{tX_i}\right]=p_ie^t+(1-p_i)=1+p_i(e^t-1)\leq e^{p_i(e^t-1)}\end{equation}</div>
<p>
这里的放缩又利用了<span class="math">\(1+\alpha \leq e^{\alpha}\)</span>。最终，我们将每一个分量都带入，得到</p>
<div class="math">\begin{equation}\begin{split}Pr[X&gt;(1+\delta)\mu]&amp;\leq  e^{-t(1+\delta)\mu}\mathbb{E}[e^{tX}]=e^{-t(1+\delta)\mu}\prod {\mathbb{E}\left[e^{tX_i}\right]}\\&amp;\leq e^{-t(1+\delta)\mu}\prod {e^{p_i(e^t-1)}}\leq e^{-t(1+\delta)\mu}e^{\mu(e^t-1))}\end{split}\end{equation}</div>
<p>
最后，我们令<span class="math">\(t=ln(1+\delta)\)</span>，化简得到</p>
<div class="math">\begin{equation}Pr[X&gt;(1+\delta)\mu]\leq \left[ \frac{e^{\delta}}{{(1+\delta)}^{(1+\delta)}}\right]^{\mu}\end{equation}</div>
<p>
这个式子提供了一个上界。对称的来说，对于下界我们也有一个不等式。
假设<span class="math">\(X_1,X_2,\cdots,X_n\)</span>为符合bernoulli分布的各自独立的随机变量，<span class="math">\(X=\sum {X_i}\)</span>，且<span class="math">\(\mu \le \mathbb{E}[X]\)</span>。则对于任意的<span class="math">\(1&gt;\delta &gt;0\)</span>，下面的不等式都成立</p>
<div class="math">\begin{equation}Pr[X&lt;(1-\delta)\mu]&lt; \left[ \frac{e^{-\delta}}{{(1-\delta)}^{(1-\delta)}}\right]^{\mu}\end{equation}</div>
<p>
这个公式的证明类似于之前公式的证明，这里我们要借助一个<span class="math">\(t&lt;0\)</span>。</p>
<div class="math">\begin{equation}\begin{split}Pr[X&lt;(1-\delta)\mu]&amp;=Pr[tX&gt;t(1-\delta)\mu]\leq e^{(-t(1-\delta)\mu)}\mathbb{E}(e^{tX})\\&amp;=e^ {(-t(1-\delta)\mu)}\prod {\mathbb{E}(e^{tX_i})}\end{split}\end{equation}</div>
<p>
这里我们再次遇到<span class="math">\(\mathbb{E}(tX)\)</span>，再次利用Bernoulli分布性质，假设<span class="math">\(X_i=1\)</span>的概率为<span class="math">\(p_i\)</span>，则有</p>
<div class="math">\begin{equation}\mathbb{E}(e^{tX_i})=p_ie^t+(1-p_i)\leq e^{p_i(e^t-1)}\end{equation}</div>
<p>
将这个不等式带入，得到</p>
<div class="math">\begin{equation}\begin{split}Pr[X&lt;(1-\delta)\mu]&amp;\leq e^{(-t(1-\delta)\mu)}\prod {\mathbb{E}(e^{tX_i})}\\&amp;\leq e^ {(-t(1-\delta)\mu)}e^{\mathbb{E}(X)(e^t-1)}\\&amp;\leq e^{(-t(1-\delta)\mu)}e^{\mu(e^t-1)}\end{split}\end{equation}</div>
<p>
在这里，我们让<span class="math">\(t=ln(1-\delta)\)</span>，代入得</p>
<div class="math">\begin{equation}Pr[X&lt;(1-\delta)\mu]\le \left[ \frac{e^{-\delta}}{{(1-\delta)}^{(1-\delta)}}\right]^{\mu}\end{equation}</div>
<p>
这样，下界我们也证明了。上下界一起大概就是中心极限定理的内容了。实际上，这两个界并不怎么好用，光写起来就十分麻烦，我们要对这两个界进一步放缩。那么问题来了，放缩方法哪家强，当然是Taylor展开了。我们可以观察到</p>
<div class="math">\begin{equation}ln(1+\delta)=\sum (-1)^{i+1}\frac{{\delta}^i}{i}\end{equation}</div>
<p>
所以,当<span class="math">\(\delta&lt;1\)</span>时</p>
<div class="math">\begin{equation}(1+\delta)ln(1+\delta)=\delta +\sum_{i\geq 2}{{(-\delta)}^i\{\frac{1}{i-1}-\frac{1}{i}\}}\geq \delta +{\delta}^2/2-{\delta}^3/6\geq \delta+{\delta}^2/3\end{equation}</div>
<p>
综上我们有</p>
<div class="math">\begin{equation}Pr[X&gt;(1+\delta)\mu]\leq e^{\frac{-{\delta}^2\mu}{3}} \quad (0&lt;\delta&lt;1)\end{equation}</div>
<p>
对称的，我们也有</p>
<div class="math">\begin{equation}Pr[X&lt;(1-\delta)\mu]\leq e^{\frac{-{\delta}^2\mu}{2}}\quad (0&lt;\delta&lt;1)\end{equation}</div>
<h1 id="f566b1">Hoeffding不等式</h1>
<p>假设<span class="math">\(X_1,\cdots ,X_n\)</span>是<span class="math">\(n\)</span>个相互独立的随机变量，且<span class="math">\(X_i\in[a_i,b_i]\)</span>，令<span class="math">\(X=\sum {X_i}/n\)</span>，则对任意的<span class="math">\(\epsilon &gt;0\)</span>有</p>
<div class="math">\begin{equation}P\left(|X-\mathbb{E}[{X}]|&gt;\epsilon\right)\leq 2\exp\left(\frac{2n^2\epsilon^2}{\sum_{i=1}^n(b_i-a_i)^2}\right) \end{equation}</div>
<p>
看到这个公式，是不是感觉自己选择了Hard模式啊。根据对称性，Chernoff界以及随机变量的独立性质，我们可以得到</p>
<div class="math">\begin{equation}\begin{array}{rl} &amp;P\left(|{X}-\mathbb{E}[{X}]|&gt;\epsilon\right)\\ =&amp;2P\left({X}-\mathbb{E}[{X}]&gt;\epsilon\right)\\ \leq&amp; 2\underset{t\geq 0}{\inf}\exp(-t\epsilon)\mathbb{E}[\exp(t{X})]\\ =&amp; 2\underset{t\geq 0}{\inf}\exp(-t\epsilon)\mathbb{E}[\exp(t\sum_{i=1}^nX_i/n)]\\ =&amp; 2\underset{t\geq 0}{\inf}\exp(-t\epsilon)\mathbb{E}[\prod_{i=1}^n\exp(tX_i/n)]\\ =&amp; 2\underset{t\geq 0}{\inf}\exp(-t\epsilon)\prod_{i=1}^n\mathbb{E}[\exp(tX_i/n)]\\ \leq &amp; 2\underset{t\geq 0}{\inf}\exp(-t\epsilon)\prod_{i=1}^n\exp(t^2(b_i-a_i)^2/(8n^2))\\ =&amp; 2\underset{t\geq 0}{\inf}\exp(-t\epsilon+\sum_{i=1}^nt^2(b_i-a_i)^2/(8n^2)) \end{array} \end{equation}</div>
<p>
为了进一步化简结果，我们需要对右边的求极值。右边指数项里面是一个一元二次函数，求极值很简单，求一阶导数就可以了。我们定义函数<span class="math">\(p:\mathbb{R}\rightarrow\mathbb{R}\)</span>为</p>
<div class="math">\begin{equation}p(t)=-t\epsilon+\sum_{i=1}^n\frac{t^2(b_i-a_i)^2}{8n^2} \end{equation}</div>
<p>
对其求一阶导数</p>
<div class="math">\begin{equation}p'(t)=-\epsilon+\frac{t\sum_{i=1}^n(b_i-a_i)^2}{4n^2}=0 \end{equation}</div>
<p>
可得到</p>
<div class="math">\begin{equation}t=\frac{4n^2\epsilon}{\sum_{i=1}^n(b_i-a_i)^2} \end{equation}</div>
<p>
最后将这个<span class="math">\(t\)</span>带入到<span class="math">\(p(t)\)</span>即可得到Hoeffding不等式</p>
<div class="math">\begin{equation}P\left(|\bar{X}-\mathbb{E}[\bar{X}]|&gt;\epsilon\right)\leq 2\exp\left(-\frac{2n^2\epsilon^2}{\sum_{i=1}^n(b_i-a_i)^2}\right)\end{equation}</div>
<h1 id="f691c1">联合界Union Bound</h1>
<p>联合界理解起来很简单，对于有限个事件<span class="math">\(A_1,A_2,\cdots,A_n\)</span>，有</p>
<div class="math">\begin{equation}P\left(\bigcup_{i=1}^nA_i\right)\leq\sum_{i=1}^nP(A_i) \end{equation}</div>
<p>
这个不等式是通过数学归纳法来证明的，对于<span class="math">\(n=2\)</span>的时候就是</p>
<div class="math">\begin{equation}P(A\cup B)=P(A)+P(B)-P(A\cap B)\end{equation}</div>
<p>
进行归纳</p>
<div class="math">\begin{equation}\begin{array}{rl} P\left(\bigcup_{i=1}^{k+1}A_i\right)&amp;=P\left(\bigcup_{i=1}^{k}A_i\right)+P(A_{k+1})-P\left(\bigcup_{i=1}^{k}A_i\cap A_{k+1}\right)\\ &amp;\leq\sum_{i=1}^kP(A_i)+P(A_{k+1})=\sum_{i=1}^{k+1}P(A_i) \end{array} \end{equation}</div>
<p>
因此，这个联合界成立。</p>
<h1 id="cb6253">经验风险最小化</h1>
<p>机器学习求解的假设函数不是单纯为了拟合已有的数据，其最终目标是准确预测未知数据的输出。经验风险最小化存在过拟合的风险，其目的在于找到一个可以很好与训练数据匹配的假设函数，而该假设函数不一定能预测未知数据的输出，即泛化误差很大。增加训练数据的规模有利于降低泛化误差。但也不是训练数据越多越好，因为当训练数据达到一定规模后，泛化误差的变化趋于平缓。而且很多时候我们并不能拿到这么多的数据，还有可能我们的资源无法对这么多的数据进行处理。在固定了数据量的情况下，我们只能从模型复杂度下手来减少泛化误差，模型复杂度太大或者太小都会导致泛化误差过大。选择模型，就是我们预先提供的一些分布函数的集合<span class="math">\(\mathcal{H}\)</span>，这个集合包括模型<span class="math">\(\mathcal{H}=\{h_1,h_2,\cdots,h_{|\mathcal{H}|}\)</span>。根据经验风险最小化原则，我们选择模型的准则就是训练误差最小</p>
<div class="math">\begin{equation}\hat{h}=arg\underset{h_i\in\mathcal{H}}{\min}\hat{R}(h_i) \end{equation}</div>
<p>
我们需要证明我们的选择是有道理的，即使训练误差较小的假设函数<span class="math">\(\hat{h}\)</span>，他的泛化误差也不至于太大。</p>
<p>首先，对任意假设函数<span class="math">\(h\)</span>而言，<span class="math">\(\hat{R}(h)\)</span>都是<span class="math">\(R(h)\)</span>的一个可靠的近似值；其次，给出<span class="math">\(\hat{h}\)</span>的泛化误差<span class="math">\(R(\hat{h})\)</span>的上界。我们在样本空间内格局样本的概率分布<span class="math">\(\mathcal{D}\)</span>进行采样，得到训练集<span class="math">\(\mathcal{S}=\{(x^{(i)},y^{(i)})\}_{i=1}^m\)</span>。给定任意假设函数<span class="math">\(h_i\in \mathcal{H}\)</span>，定义服从Bernoulli分布的随机变量<span class="math">\(Z_j=I\{h_i(x^{(j)})\neq y^{(j)}\}\in\{0,1\}\)</span>，则<span class="math">\(Z_j\)</span>表示<span class="math">\(h_i\)</span>是否将样本<span class="math">\((x^{(j)},y^{(j)})\)</span>误分类了，<span class="math">\(h_i\)</span>的泛化误差<span class="math">\(R(h_i)=P(z_j=1)\)</span>。因为所有样本都是从同一个概率分布中独立采样的，所以<span class="math">\(\{Z_j\}_{j=1}^m\)</span>中的都是满足独立同分布的Bernoulli随机变量。<span class="math">\(h_i\)</span>的训练误差形式为</p>
<div class="math">\begin{equation}\hat{R}(h_i)=\frac{1}{m}\sum_{j=1}^mI\{h_i(x^{(j)})\neq y^{(j)}\}=\frac{1}{m}\sum_{j=1}^mz_j \end{equation}</div>
<p>显然<span class="math">\(\hat{R}(h_i)\)</span>为<span class="math">\(m\)</span>个独立同分布的Bernoulli随机变量的均值，且这些随机变量的期望值都是<span class="math">\(R(h_i)\)</span>。根据Hoeffding不等式，得到如下不等式</p>
<div class="math">\begin{equation} P\left(|R(h_i)-\hat{R}(h_i)|&gt;\gamma\right)\leq 2\exp(-2\gamma^2m) \end{equation}</div>
<p>
上述规律说明：对于特定假设函数<span class="math">\(h_i\)</span>而言，在样本数目<span class="math">\(m\)</span>足够大时，训练误差和泛化误差近似相等的概率可以非常大，这也证明了<span class="math">\(\hat{R}(h)\)</span>是<span class="math">\(R(h)\)</span>的一个可靠的近似值。但我们想要保证的不仅仅是这种近似的可靠性只对特定的假设函数成立，而是对假设集合<span class="math">\(\mathcal{H}\)</span>所有的假设函数都成立。现将时间<span class="math">\(A_i\)</span>定义为<span class="math">\(|R(h_i)-\hat{R}(h_i)|&gt;\gamma\)</span>，则由上一个结论可得<span class="math">\(P(A_i)\leq 2\exp(-2\gamma^2m)\)</span>。由于</p>
<div class="math">\begin{equation}\begin{array}{rl} &amp;P\left(\exists h\in\mathcal{H}.|R(h_i)-\hat{R}(h_i)|&gt;\gamma\right)\\ =&amp;P(\bigcup_{i=1}^{|\mathcal{H}|}A_i)\\ \leq &amp; \sum_{i=1}^{|\mathcal{H}|}P(A_i)\\ \leq &amp; 2\sum_{i=1}^{|\mathcal{H}|}\exp(-2\gamma^2m)\\ =&amp; 2|\mathcal{H}|\exp(-2\gamma^2m) \end{array} \end{equation}</div>
<p>
因此</p>
<div class="math">\begin{equation}\begin{array}{rl} &amp;P\left(\neg\exists h\in\mathcal{H}.|R(h_i)-\hat{R}(h_i)|&gt;\gamma\right)\\ =&amp;P\left(\forall h\in\mathcal{H}.|R(h_i)-\hat{R}(h_i)|\leq\gamma\right)\\ \geq &amp; 1-2|\mathcal{H}|\exp(-2\gamma^2m) \end{array}\end{equation}</div>
<p>
上式被称为一致性收敛(uniform convergence)，因为这个界对于<span class="math">\(\mathcal{H}\)</span>都成立。由此可知，<span class="math">\(\mathcal{H}\)</span>中所有假设函数的训练误差与泛化误差偏离范围在<span class="math">\(\gamma\)</span>内的概率至少为</p>
<div class="math">\begin{equation}1-2|\mathcal{H}|\exp(-2\gamma^2m)\end{equation}</div>
<p>
在给定其中两个值得情况下，我们可以推出第三个变量需要满足的条件。例如为了确保所有假设函数的训练误差和泛化误差间的偏差范围都在γ<span class="math">\(\gamma\)</span>以内的概率至少为<span class="math">\(1-\delta\)</span>，至少需要多少个训练样本？这个问题对应的数学表述为： </p>
<div class="math">\begin{equation}P\left(\forall h\in\mathcal{H}.|R(h_j)-\hat{R}(h_j)|\leq\gamma\right)\geq 1-2|\mathcal{H}|\exp(-2\gamma^2m)\geq 1-\delta \end{equation}</div>
<p>
由此我们可以得到训练样本数目的下界为</p>
<div class="math">\begin{equation}m\geq\frac{1}{2\gamma^2}\log\frac{2|\mathcal{H}|}{\delta}=O\left(\frac{1}{\gamma^2}\log\frac{|\mathcal{H}|}{\delta}\right) \end{equation}</div>
<p>
这条规律对我们是非常有用的，由此我们可以推测算法性能要达到某种水平，至少需要多少个样本就足够了，这也称为算法的样本复杂度(sample complexity)。一般而言，<span class="math">\(\log|\mathcal{H}|\)</span>增长很慢，据CMU的Andrew Moore所说，<span class="math">\(\log|\mathcal{H}|\leq 30\)</span>。样本数目太少不能保证训练出来的模型有较优的性能，样本数目也没必要太多太多，尤其在样本的收集比较困难的情况下，只要能在模型性能达到期望的效果，样本越少越好。 同样地，假设样本数目<span class="math">\(m\)</span>和<span class="math">\(\delta\)</span>都确定了，我们想知道假设集合<span class="math">\(\mathcal{H}\)</span>中所有假设函数的训练误差和泛化误差间偏离程度<span class="math">\(\gamma\)</span></p>
<div class="math">\begin{equation}|R(h_j)-\hat{R}(h_j)|\leq\gamma\leq\sqrt{\frac{1}{2m}\log\frac{2|\mathcal{H}|}{\delta}} \end{equation}</div>
<p>
接下来，我们给出<span class="math">\(\hat{h}\)</span>的泛化误差上界。定义假设集合<span class="math">\(\mathcal{H}\)</span>的最优假设函数为</p>
<div class="math">\begin{equation}h^{\star}=arg\underset{h\in\mathcal{H}}{\min}\;R(h) \end{equation}</div>
<p>
由于<span class="math">\(h^{\star}\)</span>训练误差小于<span class="math">\(\hat{h}\)</span>，结合<span class="math">\(|R(h)-\hat{R}{h}|\leq\gamma\)</span>，可得到<span class="math">\(\hat{h}\)</span>的泛化误差上界：</p>
<div class="math">\begin{equation}R(\hat{h})\leq\hat{R}(\hat{h})+\gamma\leq\hat{R}(h^{\star})+\gamma\leq R(h^{\star})+2\gamma\end{equation}</div>
<p>
上式说明，训练误差最小的假设函数<span class="math">\(\hat{h}\)</span>的泛化误差顶多比假设集合中最优的假设函数<span class="math">\(h^{\star}\)</span>高<span class="math">\(2\gamma\)</span>，即：</p>
<div class="math">\begin{equation}R(\hat{h})\leq\left(\underset{h\in\mathcal{H}}{\min}\;R(h)\right)+2\sqrt{\frac{1}{2m}\log\frac{2|\mathcal{H}|}{\delta}} \end{equation}</div>
<p>
我们可以总结出如下规律：在<span class="math">\(|\mathcal{H}|\)</span>有限的情况下，只要样本数目<span class="math">\(m\)</span>足够大，就有<span class="math">\(R(\hat{h})\approx \hat{R}(h)\)</span>；此时若有<span class="math">\(\hat{R}(\hat{h } )\approx 0\)</span>，必然有<span class="math">\(R(\hat{h} )\approx 0\)</span>，即通过经验风险最小化得到的最优假设函数是可靠的。但实际情况并非如此简单：如果<span class="math">\(|\mathcal{H}|\)</span>有限，在大多数情况下我们无法保证从<span class="math">\(\mathcal{H}\)</span>中选择出来的假设函数的经验风险趋近于0(少数时候可以碰巧找到经验风险为0的假设函数)，此时<span class="math">\(\hat{h}\)</span>的泛化误差可能无法令人满意；如果|<span class="math">\(\mathcal{H}\)</span>趋于无限，我们虽可以保证<span class="math">\(R(\hat{h} )\approx 0\)</span>，但根据上式又无法得到<span class="math">\(R(\hat{h})\approx \hat{R}(h)\)</span>，使得无法确保<span class="math">\(\hat{h}\)</span>的泛化误差足够小。</p>
<h1 id="4677be">SVM的置信界</h1>
<p>假设训练点都处于一个半径为<span class="math">\(R\)</span>的球体之中，且让<span class="math">\(G(x)=\text{sign}[f(x)]=\text{sign}[{\beta}^Tx+{\beta}_o]\)</span>,则由<span class="math">\({G(x),\Vert\beta\Vert\leq A}\)</span>组成 的函数集合的VC维<span class="math">\(h\)</span>满足</p>
<div class="math">\begin{equation}h\leq R^2A^2\end{equation}</div>
<p>
如果<span class="math">\(f(x)\)</span>是在<span class="math">\(\Vert \beta\Vert\leq A\)</span>下分离训练数据最优的函数。则在测试集上的误差<span class="math">\(E(test)\)</span>我们有如下界限</p>
<div class="math">\begin{equation}Pr(E(test)\leq 4\frac{h[\log (2B/h)+1]-\log (\eta /4)}{N})\geq 1-\eta\end{equation}</div>
<p>
在SVM中，规则化参数<span class="math">\(C\)</span>的作用是用来约束分类器VC维的上界。根据结构风险最小化原则，我们来选择使得测试误差上界最小的<span class="math">\(C\)</span>。但是利用正则化项<span class="math">\(C\)</span>与交叉验证相比哪个更好，目前还不清楚。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
    <div id="content-comments">
</div>

</div>
<div class="niu2-right-container col-md-2">
    <div id="niu2-sidebar-meta" class="niu2-sidebar">
        <div class="niu2-sidebar-label"><i class="icon-calendar"></i>Published:</div>
        <div class="niu2-sidebar-value">2015-04-17 21:33</div>
        <div class="niu2-sidebar-label"><i class="icon-open-folder"></i>Category:</div>
        <div class="niu2-sidebar-value"><a href="./category/math.html">Math</a></div>
        <div class="niu2-sidebar-label"><i class="icon-tag"></i>Tag:</div>
 
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="./tag/chernoff.html">Chernoff</a><sup>1</sup></div>
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="./tag/ai.html">AI</a><sup>5</sup></div>
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="./tag/converge.html">Converge</a><sup>5</sup></div>
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="./tag/math.html">Math</a><sup>6</sup></div>
    </div>

    <div id="niu2-sidebar-toc" class="niu2-sidebar" data-status="closed">
        <div class="niu2-sidebar-label">
            <i id="niu2-sidebar-toc-ctrl" class="icon-open-tocs"></i>TOC
        </div>
        <ol id="niu2-sidebar-toc-list">
            <li><a href="#content-heading">The Chernoff Bound</a></li>
            <li><a href='#7d0e78'>在期望已知的情况下Chernoff Bound的变体</a></li><li><a href='#8fe964'>在bernoulli分布中的Chernoff Bound</a></li><li><a href='#f566b1'>Hoeffding不等式</a></li><li><a href='#f691c1'>联合界Union Bound</a></li><li><a href='#cb6253'>经验风险最小化</a></li><li><a href='#4677be'>SVM的置信界</a></li>
        </ol>
    </div>
</div>    </div>

    <div class="niu2-footer">
        <div id="body-footer" class="col-md-6 col-md-offset-2">
<hr/>
<p>
    Powered by <a href="https://github.com/getpelican/pelican">Pelican</a>, 
    <a href="https://github.com/mawenbao/niu-x2-sidebar">theme</a> built with <a href="http://getbootstrap.com">Bootstrap3</a>
    by <a href="http://blog.atime.me">Ma Wenbao</a>, icons by 
    <a href="http://fortawesome.github.io/Font-Awesome">Font Awesome</a>.
</p>
<p>
    ©
            2014-2016
    <a class="niu2-footer-link" href=".">Huiliang Huang</a>
</p>
<p class="niu2-icons">
</p>        </div>
    </div>
    
    <div id="niu2-pygments" data-theme="github"></div>

    <script type="text/javascript" src="./theme/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="./theme/js/niu2.min.js"></script>
    <script type="text/javascript" src="./theme/js/bootstrap.min.js"></script>
    <script type="text/javascript">onContentLoaded();</script>
  </body>
</html>