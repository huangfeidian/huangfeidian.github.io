<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Under the Hood : Caffe - Spirits Away</title>
    <link rel="shortcut icon" type="image/x-icon" href="\favicon.ico">
    <link rel="stylesheet" href="http://spiritsaway.info/theme/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="http://spiritsaway.info/theme/font-icons/style.min.css" type="text/css" />
    <link rel="stylesheet" href="http://spiritsaway.info/theme/css/niu2.min.css" type="text/css" />

    <link rel="canonical" href="http://spiritsaway.info/code/under-the-hood-caffe.html" />
    
    <script type="text/javascript">window.onload=function(){};</script>
    <!--[if lt IE 9]>
        <script src="http://spiritsaway.info/theme/js/html5shiv.js"></script>
        <script src="http://spiritsaway.info/theme/js/respond.min.js"></script>
    <![endif]-->
  </head>
  <body> 
    <div id="body-header">
<div class="navbar navbar-inverse navbar-fixed-top">
<div class="col-md-12">
    <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="http://spiritsaway.info">
            <i class="icon-home"></i>Spirits Away
        </a>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
        <ul class="nav navbar-nav">
            <li><a href="/about.html" title="about">
                <i class="fa-anchor"></i>关于</a>
            </li>
            <li><a href="/archives.html" title="archives">
                <i class="fa-archive"></i>存档</a>
            </li>
            <li><a href="/tag/" title="tags">
                <i class="fa-tag"></i>标签</a>
            </li>
        <!-- category dropdown list -->
        <li class="dropdown">
           <a href="#" class="dropdown-toggle" data-toggle="dropdown">
                <i class="icon-folder-open"></i>分类<b class="caret"></b>
           </a>
           <ul class="dropdown-menu">
               <li><a href="http://spiritsaway.info/ai/index.html">
                   <i class="icon-folder-close"></i>AI
                      (3)</a></li>
               <li><a href="http://spiritsaway.info/algorithm/index.html">
                   <i class="icon-folder-close"></i>Algorithm
                      (3)</a></li>
               <li><a href="http://spiritsaway.info/code/index.html">
                   <i class="icon-folder-close"></i>Code
                      (3)</a></li>
               <li><a href="http://spiritsaway.info/cpp/index.html">
                   <i class="icon-folder-close"></i>CPP
                      (4)</a></li>
               <li><a href="http://spiritsaway.info/math/index.html">
                   <i class="icon-folder-close"></i>Math
                      (4)</a></li>
           </ul>
        </li>
        <!--  self defined dropdown list -->
        </ul>
        
        <!-- right nav bar -->
        <ul class="nav navbar-nav navbar-right">
        <!-- google custom search -->
       </ul>
    </nav>
</div>
</div>    </div>
    
    <div id="body-content">
<div class="col-md-8 col-md-offset-2">
    <h1 id="content-heading">Under the Hood : Caffe</h1>
</div>
<div id="niu2-left-container" class="col-md-6 col-md-offset-2 with-right-border">
    <div id="niu2-main-content">
        <h1 id="9fc63e">device_alternate.hpp</h1>
<p>It diffrentiate the CPU only mode and GPCPU mode with <code>ifdef CPU_ONLY</code>. In the <code>CPU_ONLY</code> closure, some functions are stubed,like</p>
<ul>
<li>
<p><code>template &lt;typename Dtype&gt; void classname&lt;Dtype&gt;::Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;\&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;\&amp; top)</code>;</p>
</li>
<li>
<p><code>template &lt;typename Dtype&gt; void classname&lt;Dtype&gt;::Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;\&amp; top, const vector&lt;bool&gt;\&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;\&amp; bottom)</code>;</p>
</li>
<li>
<p><code>template &lt;typename Dtype&gt; void classname&lt;Dtype&gt;::funcname\#\#_\#\#gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;\&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;\&amp; top)</code>;</p>
</li>
<li>
<p><code>template &lt;typename Dtype&gt; void classname&lt;Dtype&gt;::funcname\#\#_\#\#gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;\&amp; top, const vector&lt;bool&gt;\&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;\&amp; bottom)</code>.</p>
</li>
</ul>
<p>Personally, I don't like these macro trick like ## which concates strings. When macro and template mess up, It's very hard to find what goes wrong.</p>
<p>If <code>CPU_ONLY</code> is not defined, there goes the mix mode. The device_alternate.hpp introduce all the cuda headers and some CUDA_CHECK macro,like</p>
<ul>
<li>
<p><code>CUDA_CHECK(condition)</code>,</p>
</li>
<li>
<p><code>CUBLAS_CHECK(condition)</code>,</p>
</li>
<li>
<p><code>CURAND_CHECK(condition)</code>,</p>
</li>
</ul>
<p>Aside from these CHECK macro, it introduce a CUDA_KERNEL_LOOP macro:</p>
<div class="codehilite"><pre><span class="cp">#define CUDA_KERNEL_LOOP(i, n) \</span>
<span class="cp">  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \</span>
<span class="cp">       i &lt; (n); \</span>
<span class="cp">       i += blockDim.x * gridDim.x)</span>
</pre></div>


<p>At the end of this file, it defines a block dispatch function which determines how many blocks every thread would consume:</p>
<div class="codehilite"><pre><span class="kr">inline</span> <span class="kt">int</span> <span class="nf">CAFFE_GET_BLOCKS</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">CAFFE_CUDA_NUM_THREADS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">CAFFE_CUDA_NUM_THREADS</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>


<p>The <code>CAFFE_CUDA_NUM_THREADS</code> depends on the <code>_CUDA_ARCH</code>: if the value is no less than 200, then the number of cuda threads is 1024, otherwise 512.</p>
<h1 id="0e4f38">proto/caffe.pb.h</h1>
<p>This file is generated by Google protobuf. It has all the definition of data stream structure and some get/set/serialize functions. Just the first glance of this header file, my eyes burn. Maybe my cat is lost in it.</p>
<h1 id="23937f">util/rng.hpp</h1>
<p>This header defines two functions, one called caffe_rng which return a rng_seed(boost::mt19937),the other one is a template shuffle functions. Actually these two functions just make no sense, why not just use c++11.</p>
<h1 id="ad7c65">util/mkl_alternate.h</h1>
<p>This header file works just like device_alternate.hpp, it diffrentiate MKL and other BLAS librarys. If we compile caffe with mkl, then there is nothing to do because MKL already define all the functions we need. Otherwise, we define some functions with macro. The macro works as below:</p>
<div class="codehilite"><pre><span class="cp">#define DEFINE_VSL_UNARY_FUNC(name, operation) \</span>
<span class="cp">  template&lt;typename Dtype&gt; \</span>
<span class="cp">  void v##name(const int n, const Dtype* a, Dtype* y) { \</span>
<span class="cp">    CHECK_GT(n, 0); CHECK(a); CHECK(y); \</span>
<span class="cp">    for (int i = 0; i &lt; n; ++i) { operation; } \</span>
<span class="cp">  } \</span>
<span class="cp">  inline void vs##name( \</span>
<span class="cp">    const int n, const float* a, float* y) { \</span>
<span class="cp">    v##name&lt;float&gt;(n, a, y); \</span>
<span class="cp">  } \</span>
<span class="cp">  inline void vd##name( \</span>
<span class="cp">      const int n, const double* a, double* y) { \</span>
<span class="cp">    v##name&lt;double&gt;(n, a, y); \</span>
<span class="cp">  }</span>
</pre></div>


<p>It introduce a template then instantiate the template with <code>float</code> and <code>double</code>, where vs stands for float vector and vd stands for double vector. After the macro definition, this file predefine some simple unary functions:</p>
<div class="codehilite"><pre><span class="n">DEFINE_VSL_UNARY_FUNC</span><span class="p">(</span><span class="n">Sqr</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="n">DEFINE_VSL_UNARY_FUNC</span><span class="p">(</span><span class="n">Exp</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span>
<span class="n">DEFINE_VSL_UNARY_FUNC</span><span class="p">(</span><span class="n">Abs</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">fabs</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span>
</pre></div>


<p>Aside from the vector transform template, this file also introduce a template which transform vector with additional parameter and atemplate of binary function:</p>
<div class="codehilite"><pre><span class="cp">#define DEFINE_VSL_UNARY_FUNC_WITH_PARAM(name, operation) \</span>
<span class="cp">  template&lt;typename Dtype&gt; \</span>
<span class="cp">  void v##name(const int n, const Dtype* a, const Dtype b, Dtype* y) { \</span>
<span class="cp">    CHECK_GT(n, 0); CHECK(a); CHECK(y); \</span>
<span class="cp">    for (int i = 0; i &lt; n; ++i) { operation; } \</span>
<span class="cp">  } </span>
<span class="cp">#define DEFINE_VSL_BINARY_FUNC(name, operation) \</span>
<span class="cp">  template&lt;typename Dtype&gt; \</span>
<span class="cp">  void v##name(const int n, const Dtype* a, const Dtype* b, Dtype* y) { \</span>
<span class="cp">    CHECK_GT(n, 0); CHECK(a); CHECK(b); CHECK(y); \</span>
<span class="cp">    for (int i = 0; i &lt; n; ++i) { operation; } \</span>
<span class="cp">  }</span>
</pre></div>


<p>And then instantiate these template with some simple functions,like:</p>
<div class="codehilite"><pre><span class="n">DEFINE_VSL_BINARY_FUNC</span><span class="p">(</span><span class="n">Add</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="p">;</span>
<span class="n">DEFINE_VSL_BINARY_FUNC</span><span class="p">(</span><span class="n">Sub</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="p">;</span>
<span class="n">DEFINE_VSL_BINARY_FUNC</span><span class="p">(</span><span class="n">Mul</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="p">;</span>
<span class="n">DEFINE_VSL_BINARY_FUNC</span><span class="p">(</span><span class="n">Div</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="p">;</span>
</pre></div>


<p>In addition, MKL comes with an additional function axpby that is not present in standard blas. caffe  simply use a two-step (inefficient, of course) way to mimic that.</p>
<div class="codehilite"><pre><span class="kr">inline</span> <span class="kt">void</span> <span class="nf">cblas_saxpby</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">X</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">incX</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">incY</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">cblas_sscal</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">incY</span><span class="p">);</span>
    <span class="n">cblas_saxpy</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">incX</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">incY</span><span class="p">);</span>
<span class="p">}</span>
<span class="kr">inline</span> <span class="kt">void</span> <span class="nf">cblas_daxpby</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">double</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">double</span><span class="o">*</span> <span class="n">X</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">incX</span><span class="p">,</span> <span class="k">const</span> <span class="kt">double</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">incY</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">cblas_dscal</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">incY</span><span class="p">);</span>
    <span class="n">cblas_daxpy</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">incX</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">incY</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>where cblas_sscal, cblas_saxpy may  defined as such:</p>
<div class="codehilite"><pre><span class="kt">void</span> <span class="nf">cblas_sscal</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="kt">int</span> <span class="n">incY</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">incY</span><span class="p">]</span><span class="o">*=</span><span class="n">beta</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="nf">cblas_saxpy</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="kt">int</span> <span class="n">incX</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span> <span class="n">Y</span><span class="p">,</span><span class="kt">int</span> <span class="n">incY</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">incY</span><span class="p">]</span><span class="o">=</span><span class="n">alpha</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">incX</span><span class="p">]</span><span class="o">+</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">incY</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>So the saxpby is essentially <span class="math">\(y=a*x+b*y\)</span>.</p>
<h1 id="83fcbe">util/math_fucntions</h1>
<p>In this header file, caffe declares all the BLAS functions(the cpu version and gpu version) the CNN needs. Here are some interesting function names without device prefix:</p>
<ul>
<li>
<p>gemm, which stands for general matrix matrix multiply,</p>
</li>
<li>
<p>gemv, which stands for general matrix vector multiply,</p>
</li>
<li>
<p>axpy, which stands for <span class="math">\(y=a*x+y\)</span>,</p>
</li>
<li>
<p>axpby, which stands for <span class="math">\(y=a*x+b*y\)</span>,</p>
</li>
<li>
<p>scal, which stands for<span class="math">\(y=a*y\)</span>,</p>
</li>
<li>
<p>asum, which stands for <span class="math">\(\sum{abs(y)}\)</span>,</p>
</li>
<li>
<p>stride_dot, which stands for <span class="math">\(\sum{y[i*incY]*x[i*incX]}\)</span>,</p>
</li>
</ul>
<p>These function would call respective cblas or cuda_blas. So these declarations are just encapsulation.  Aside from these blas functions, some random generator(gaussian and uniform) and simple math function (like div,sub )are declared.All the definition details are in the math_functions.cu and math_functions.cpp.</p>
<h1 id="16cd7a">common.hpp</h1>
<p>This header file defines some common macro and the caffe class. The caffe class is a singleton class which ia accessed by the <code>get</code> fucntion.</p>
<div class="codehilite"><pre><span class="kr">inline</span> <span class="k">static</span> <span class="n">Caffe</span><span class="o">&amp;</span> <span class="n">Get</span><span class="p">()</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">singleton_</span><span class="p">.</span><span class="n">get</span><span class="p">())</span>
    <span class="p">{</span>
        <span class="n">singleton_</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">Caffe</span><span class="p">());</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="o">*</span><span class="n">singleton_</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>


<p>Personally I don't like this singleton implenmentation, we can just use static to achieve the same while keep the shared_ptr away.</p>
<div class="codehilite"><pre><span class="k">static</span> <span class="n">Caffe</span><span class="o">&amp;</span> <span class="n">get</span><span class="p">()</span>
<span class="p">{</span>
    <span class="k">static</span> <span class="n">Caffe</span> <span class="n">instance</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">instance</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>


<p>There is a rng class declared in caffe to generate random numbers.</p>
<h1 id="552c74">syncedmem.hpp</h1>
<p>This header file wraps the CPU/GPU memory malloc/free function and the synchronization between GPU memory and CPU memory. The class which wraps the sync-memory is declared as such:</p>
<div class="codehilite"><pre><span class="k">class</span> <span class="nc">SyncedMemory</span>
<span class="p">{</span>
    <span class="k">private</span><span class="o">:</span>
        <span class="kt">void</span> <span class="n">to_cpu</span><span class="p">();</span>
        <span class="kt">void</span> <span class="nf">to_gpu</span><span class="p">();</span>
        <span class="kt">void</span><span class="o">*</span> <span class="n">cpu_ptr_</span><span class="p">;</span>
        <span class="kt">void</span><span class="o">*</span> <span class="n">gpu_ptr_</span><span class="p">;</span>
        <span class="kt">size_t</span> <span class="n">size_</span><span class="p">;</span>
        <span class="n">SyncedHead</span> <span class="n">head_</span><span class="p">;</span>
        <span class="kt">bool</span> <span class="n">own_cpu_data_</span><span class="p">;</span>
        <span class="n">DISABLE_COPY_AND_ASSIGN</span><span class="p">(</span><span class="n">SyncedMemory</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>


<p>For a sync-memory, there has to be two blocks of memory, <code>cpu_ptr</code> malloced on CPU and <code>gpu_ptr</code> the other malloced on GPU, both have the size <code>size_</code>. The <code>to_cpu</code> and <code>to_gpu</code> function do the synchonization work.</p>
<p>The SyncdHead is a enum that indicates which block has the updated value, it is defined as such:</p>
<div class="codehilite"><pre><span class="k">enum</span> <span class="n">SyncedHead</span>
<span class="p">{</span>
    <span class="n">UNINITIALIZED</span><span class="p">,</span> <span class="n">HEAD_AT_CPU</span><span class="p">,</span> <span class="n">HEAD_AT_GPU</span><span class="p">,</span> <span class="n">SYNCED</span>
<span class="p">};</span>
</pre></div>


<p>The DISABLE_COPY_AND_ASSIGN macro simply declares the copy and assign function private, while template would make it more comprehensible.</p>
<p>Caffe use SyncedMem class to synchronize values between the CPU and GPU in order to hide the synchronization details and to minimize data transfer. A rule of thumb is, always use the const call if you do not want to change the values, and never store the pointers in your own object. Every time you work on a blob, call the functions to get the pointers, as the SyncedMem will need this to figure out when to copy data.</p>
<p>In practice when GPUs are present, one loads data from the disk to a blob in CPU code, calls a device kernel to do GPU computation, and ferries the blob off to the next layer, ignoring low-level details while maintaining a high level of performance. As long as all layers have GPU implementations, all the intermediate data and gradients will remain in the GPU.</p>
<h1 id="ed894f">util/db.hpp</h1>
<p>This file simply serve as a leveldb wrapper, nothing to talk about. But why does caffe need to use leveldb?</p>
<h1 id="8e17f8">util/io.hpp</h1>
<p>The io.hpp's main concern is file io, something like proto/Datum/CVMat serialization/deserialization and temp file management. The implementation are trival, so i just list the function names it delared.</p>
<ul>
<li>
<p><code>inline void MakeTempFilename(string* temp_filename)</code></p>
</li>
<li>
<p><code>inline void MakeTempDir(string* temp_dirname)</code></p>
</li>
<li>
<p><code>inline void (Read/Write)Proto(From/to)(Text/Binary)File(...)</code></p>
</li>
<li>
<p><code>inline bool Read(File/Image)ToDatum</code></p>
</li>
</ul>
<p>Currently I don't know what a datum or CVMat mean, we will see.</p>
<h1 id="e16c12">internal_thread.hpp</h1>
<p>Yet another library wrapper, just like the leveldb wrapper. This file wrapper the boost.thread to a InternalThread Class. Why not just use C++11?</p>
<h1 id="f1872c">The Main Structure of Caffe</h1>
<h1 id="a7d6d5">blob.hpp</h1>
<p>This header file defines the Blob class.A Blob is a wrapper over the actual data being processed and passed along by Caffe, and also under the hood provides synchronization capability between the CPU and the GPU. Mathematically, a blob is a 4-dimensional array that stores things in the order of (Num, Channels, Height and Width), from major to minor, and stored in a C-contiguous fashion. The main reason for putting Num (the name is due to legacy reasons, and is equivalent to the notation of “batch” as in minibatch SGD).</p>
<ul>
<li>
<p>Number is the batch size of the data. Batch processing achieves better throughput for communication and device processing. For an ImageNet training batch of 256 images Number = 256.</p>
</li>
<li>
<p>Channel is the feature dimension e.g. for RGB images Channel = 3.</p>
</li>
</ul>
<p>Caffe stores and communicates data in 4-dimensional arrays called blobs. Blobs provide a unified memory interface, holding data e.g. batches of images, model parameters, and derivatives for optimization.The conventional blob dimensions for data are number N x channel K x height H x width W. Blob memory is row-major in layout so the last / rightmost dimension changes fastest. For example, the value at index (n, k, h, w) is physically located at index ((n * K + k) * H + h) * W + w. </p>
<p>Blob stores two chunks of memories, data and diff. The former is the normal data that we pass along, and the latter is the gradient computed by the network. The two chunks of memories is declared as such:</p>
<div class="codehilite"><pre><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">SyncedMemory</span><span class="o">&gt;</span> <span class="n">data_</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">SyncedMemory</span><span class="o">&gt;</span> <span class="n">diff_</span><span class="p">;</span>
</pre></div>


<p>The memories is allocated as such:</p>
<div class="codehilite"><pre><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Reshape</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span> <span class="n">shape</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">CHECK_LE</span><span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">kMaxBlobAxes</span><span class="p">);</span>
    <span class="n">count_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">shape_</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">CHECK_GE</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">);</span>
        <span class="n">count_</span> <span class="o">*=</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
        <span class="n">shape_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">count_</span> <span class="o">&gt;</span> <span class="n">capacity_</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">capacity_</span> <span class="o">=</span> <span class="n">count_</span><span class="p">;</span>
        <span class="n">data_</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">SyncedMemory</span><span class="p">(</span><span class="n">capacity_</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Dtype</span><span class="p">)));</span>
        <span class="n">diff_</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">SyncedMemory</span><span class="p">(</span><span class="n">capacity_</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Dtype</span><span class="p">)));</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>In the blob.cpp implementation details, the most important function is <code>Update</code>. If the newest data is on GPU side, the <code>Update</code> function calls the <code>caffe_gpu_axpy</code>, otherwise calls the <code>caffe_axpy</code>.here is the definition of <code>Update</code>:</p>
<div class="codehilite"><pre><span class="kt">void</span> <span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Update</span><span class="p">()</span>
<span class="p">{</span>
<span class="c1">// We will perform update based on where the data is located.</span>
<span class="k">switch</span> <span class="p">(</span><span class="n">data_</span><span class="o">-&gt;</span><span class="n">head</span><span class="p">())</span>
<span class="p">{</span>
<span class="k">case</span> <span class="n">SyncedMemory</span><span class="o">::</span><span class="nl">HEAD_AT_CPU</span><span class="p">:</span>
    <span class="c1">// perform computation on CPU</span>
    <span class="n">caffe_axpy</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">count_</span><span class="p">,</span> <span class="n">Dtype</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Dtype</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">diff_</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">()),</span>
            <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data_</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">()));</span>
    <span class="k">break</span><span class="p">;</span>
<span class="k">case</span> <span class="n">SyncedMemory</span><span class="o">::</span><span class="nl">HEAD_AT_GPU</span><span class="p">:</span>
<span class="k">case</span> <span class="n">SyncedMemory</span><span class="o">::</span><span class="nl">SYNCED</span><span class="p">:</span>
<span class="cp">#ifndef CPU_ONLY</span>
    <span class="c1">// perform computation on GPU</span>
    <span class="n">caffe_gpu_axpy</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">count_</span><span class="p">,</span> <span class="n">Dtype</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Dtype</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">diff_</span><span class="o">-&gt;</span><span class="n">gpu_data</span><span class="p">()),</span>
            <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data_</span><span class="o">-&gt;</span><span class="n">mutable_gpu_data</span><span class="p">()));</span>
<span class="cp">#else</span>
    <span class="n">NO_GPU</span><span class="p">;</span>
<span class="cp">#endif</span>
    <span class="k">break</span><span class="p">;</span>
<span class="k">default</span><span class="o">:</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span><span class="o">&lt;&lt;</span> <span class="s">&quot;Syncedmem not initialized.&quot;</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">}</span>
</pre></div>


<h1 id="401b9c">filler.hpp</h1>
<p>The filler header file does a really simple work: fill the blob with some data. The filled data comes from:</p>
<ul>
<li>
<p>constant, 0 expected.</p>
</li>
<li>
<p>uniform distribution w.r.t. (a,b).</p>
</li>
<li>
<p>gaussian distribution w.r.t. <span class="math">\((\theta, \delta)\)</span>.</p>
</li>
<li>
<p>positive unit ball distribution</p>
</li>
<li>
<p>xavier distribution.</p>
</li>
</ul>
<p>There is a factory function to generate specific filler class from protobuf txt file:</p>
<div class="codehilite"><pre><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="n">Filler</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*</span> <span class="n">GetFiller</span><span class="p">(</span><span class="k">const</span> <span class="n">FillerParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">type</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">type</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">type</span> <span class="o">==</span> <span class="s">&quot;constant&quot;</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">ConstantFiller</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">type</span> <span class="o">==</span> <span class="s">&quot;gaussian&quot;</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">GaussianFiller</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">type</span> <span class="o">==</span> <span class="s">&quot;positive_unitball&quot;</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">PositiveUnitballFiller</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">type</span> <span class="o">==</span> <span class="s">&quot;uniform&quot;</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">UniformFiller</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">type</span> <span class="o">==</span> <span class="s">&quot;xavier&quot;</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">XavierFiller</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">else</span>
    <span class="p">{</span>
        <span class="n">CHECK</span><span class="p">(</span><span class="nb">false</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Unknown filler name: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">param</span><span class="p">.</span><span class="n">type</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">Filler</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*</span><span class="p">)</span> <span class="p">(</span><span class="nb">NULL</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<h1 id="e22d0b">layer.hpp</h1>
<p>The layer is the essence of a model and the fundamental unit of computation. Layers convolve filters, pool, take inner products, apply nonlinearities like rectified-linear and sigmoid and other elementwise transformations, normalize, load data, and compute losses like softmax and hinge.A layer takes input through bottom Blob and makes output through top Blob.</p>
<p>Each layer type defines three critical computations: setup, forward, and backward.</p>
<ul>
<li>
<p>Setup: initialize the layer and its connections once at model initialization.</p>
</li>
<li>
<p>Forward: given input from bottom compute the output and send to the top.</p>
</li>
<li>
<p>Backward: given the gradient w.r.t. the top output compute the gradient w.r.t. to the input and send to the bottom. A layer with parameters computes the gradient w.r.t. to its parameters and stores it internally.</p>
</li>
</ul>
<p>More specifically, there will be two Forward and Backward functions implemented, one for CPU and one for GPU. If you do not implement a GPU version, the layer will fall back to the CPU functions as a backup option. This may come handy if you would like to do quick experiments, although it may come with additional data transfer cost (its inputs will be copied from GPU to CPU, and its outputs will be copied back from CPU to GPU).</p>
<p>Layers have two key responsibilities for the operation of the network as a whole: a forward pass that takes the inputs and produces the outputs, and a backward pass that takes the gradient with respect to the output, and computes the gradients with respect to the parameters and to the inputs, which are in turn back-propagated to earlier layers. These passes are simply the composition of each layer’s forward and backward.</p>
<p>Layers are constructed from prototxt serialization fromat.The layer constructor take the <code>LayerParameter</code> and initiate.</p>
<div class="codehilite"><pre><span class="k">explicit</span> <span class="nf">Layer</span><span class="p">(</span><span class="k">const</span> <span class="n">LayerParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span> <span class="o">:</span>
        <span class="n">layer_param_</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// Set phase and copy blobs (if there are any).</span>
    <span class="n">phase_</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">phase</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">blobs_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">blobs_</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">blobs_size</span><span class="p">());</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">layer_param_</span><span class="p">.</span><span class="n">blobs_size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">blobs_</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">());</span>
            <span class="n">blobs_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">FromProto</span><span class="p">(</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">blobs</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>But the so-called construtor doesn't really construct the connections. It just do the blob memory allocation job. To initiate the connections, we should call the <code>virtual void LayerSetUp</code> function, whoes implemention details rely on the specific type of Layer. The layer class offer an entrance for <code>LayerSetUp</code>:</p>
<div class="codehilite"><pre><span class="kt">void</span> <span class="nf">SetUp</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
            <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">CheckBlobCounts</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span><span class="c1">//check if the batch size equals</span>
    <span class="n">LayerSetUp</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
    <span class="n">Reshape</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
    <span class="n">SetLossWeights</span><span class="p">(</span><span class="n">top</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>The <code>Reshape</code> is a virtual function as well, whichhe adjust the shapes of top blobs and internal buffers to accomodate</p>
<p>the shapes of the bottom blobs.</p>
<p>There are five kinds of layer:</p>
<ul>
<li>
<p>data layer: the data layer construct the input layer from different serialization of data, like hdf5,leveldb,lmdb etc.</p>
</li>
<li>
<p>neural layer: this kind of layer take exactly one input blob and one output blob. It's a simple transformation layer with sigmoid, tanh etc</p>
</li>
<li>
<p>common layer: Here goes the more complicated transformation, like ArgMaxLayer, ConcatLayer, FlattenLayer, SoftmaxLayer, SplitLayer etc.</p>
</li>
<li>
<p>loss layer: This layer computes the initial back-propagate gradient w.r.t. sigmoid, SquareLoss, contrastiveLoss, HingeLoss etc. </p>
</li>
<li>
<p>vision layer: This layer manipulate some specific transformation of image/vision, like convolution.</p>
</li>
</ul>
<p>All the layer details can be seen in the src/caffe/layers source files.</p>
<h1 id="4ae9e3">net.hpp</h1>
<p>The net jointly defines a function and its gradient by composition and auto-differentiation. The composition of every layer’s output computes the function to do a given task, and the composition of every layer’s backward computes the gradient from the loss to learn the task. Caffe models are end-to-end machine learning engines.</p>
<p>The net is a set of layers connected in a computation graph – a directed acyclic graph (DAG) to be exact. Caffe does all the bookkeeping for any DAG of layers to ensure correctness of the forward and backward passes. A typical net begins with a data layer that loads from disk and ends with a loss layer that computes the objective for a task such as classification or reconstruction.</p>
<p>The net is defined as a set of layers and their connections in a plaintext modeling language.The models are defined in plaintext protocol buffer schema (prototxt) while the learned models are serialized as binary protocol buffer (binaryproto) .caffemodel files.</p>
<p>Model initialization is handled by <code>Net::Init()</code>. The initialization mainly does two things: scaffolding the overall DAG by creating the blobs and layers (all resource holded by <code>shared_ptr)</code>, and calls the layers’ <code>SetUp()</code> function. It also does a set of other bookkeeping things, such as validating the correctness of the overall network architecture. Also, during initialization the Net explains its initialization by logging to INFO with the help of <code>glog</code>.</p>
<p>In the net.hpp file, a set of train/test functions are defined:</p>
<div class="codehilite"><pre><span class="n">Dtype</span> <span class="nf">ForwardFromTo</span><span class="p">(</span><span class="kt">int</span> <span class="n">start</span><span class="p">,</span> <span class="kt">int</span> <span class="n">end</span><span class="p">);</span>
<span class="n">Dtype</span> <span class="nf">ForwardFrom</span><span class="p">(</span><span class="kt">int</span> <span class="n">start</span><span class="p">);</span>
<span class="n">Dtype</span> <span class="nf">ForwardTo</span><span class="p">(</span><span class="kt">int</span> <span class="n">end</span><span class="p">);</span>
<span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">Forward</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
                                  <span class="n">Dtype</span><span class="o">*</span> <span class="n">loss</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">);</span>
<span class="kt">void</span> <span class="nf">Backward</span><span class="p">();</span>
<span class="kt">void</span> <span class="nf">BackwardFromTo</span><span class="p">(</span><span class="kt">int</span> <span class="n">start</span><span class="p">,</span> <span class="kt">int</span> <span class="n">end</span><span class="p">);</span>
<span class="kt">void</span> <span class="nf">BackwardFrom</span><span class="p">(</span><span class="kt">int</span> <span class="n">start</span><span class="p">);</span>
<span class="kt">void</span> <span class="nf">BackwardTo</span><span class="p">(</span><span class="kt">int</span> <span class="n">end</span><span class="p">)</span>
<span class="n">Dtype</span> <span class="n">ForwardBackward</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
<span class="n">Dtype</span> <span class="n">loss</span><span class="p">;</span>
<span class="n">Forward</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">loss</span><span class="p">);</span>
<span class="n">Backward</span><span class="p">();</span>
<span class="k">return</span> <span class="n">loss</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="nf">Update</span><span class="p">();</span>
</pre></div>


<h1 id="c50011">solver.hpp</h1>
<p>The solver orchestrates model optimization by coordinating the network’s forward inference and backward gradients to form parameter updates that attempt to improve the loss. The responsibilities of learning are divided between the Solver for overseeing the optimization and generating parameter updates and the Net for yielding loss and gradients.</p>
<p>The Caffe solvers are Stochastic Gradient Descent (SGD), Adaptive Gradient (ADAGRAD), and Nesterov’s Accelerated Gradient (NESTEROV).<br />
As usually, there is a factory function to generate all kinds of solver:</p>
<div class="codehilite"><pre><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*</span> <span class="n">GetSolver</span><span class="p">(</span><span class="k">const</span> <span class="n">caffe</span><span class="o">::</span><span class="n">SolverParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">caffe</span><span class="o">::</span><span class="n">SolverParameter_SolverType</span> <span class="n">type</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">solver_type</span><span class="p">();</span>

    <span class="k">switch</span> <span class="p">(</span><span class="n">type</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">case</span> <span class="nl">SolverParameter_SolverType_SGD</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">new</span> <span class="n">SGDSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
        <span class="k">case</span> <span class="nl">SolverParameter_SolverType_NESTEROV</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">new</span> <span class="n">NesterovSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
        <span class="k">case</span> <span class="nl">SolverParameter_SolverType_ADAGRAD</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">new</span> <span class="n">AdaGradSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
        <span class="k">default</span><span class="o">:</span>
            <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Unknown SolverType: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">type</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*</span><span class="p">)</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>


<p>The solver:</p>
<ul>
<li>
<p>scaffolds the optimization bookkeeping and creates the training network for learning and test network(s) for evaluation.</p>
</li>
<li>
<p>iteratively optimizes by calling forward / backward and updating parameters</p>
</li>
<li>
<p>(periodically) evaluates the test networks</p>
</li>
<li>
<p>snapshots the model and solver state throughout the optimization</p>
</li>
</ul>
<p>During each iteration:</p>
<ul>
<li>
<p>calls network forward to compute the output and loss</p>
</li>
<li>
<p>calls network backward to compute the gradients</p>
</li>
<li>
<p>incorporates the gradients into parameter updates according to the solver method</p>
</li>
<li>
<p>updates the solver state according to learning rate, history, and method</p>
</li>
</ul>
<p>to take the weights all the way from initialization to learned model.</p>
<p>Beneath the forward/backward/update  functions, what the sovler really do is to solve a optimization problem of loss minimization. We can define the problem as: to minimize </p>
<div class="math">\begin{equation}L(W)=\frac{1}{\vert D \vert}\sum_{i}^{\vert D\vert}{f_w (X^{i})} +\lambda r(W)\end{equation}</div>
<p><br />
w.r.t. dataset <span class="math">\(D\)</span>, where<span class="math">\(f_w(X^{i})\)</span> is the loss on  data instance <span class="math">\(X^i\)</span> and <span class="math">\(r(W)\)</span> is a regularization term with weight <span class="math">\(\lambda\)</span>.</p>
<p>The general optimization method is gradient descent. Because the <span class="math">\(D\)</span> in machine learning is so large that it's impossible to  calculate the global gradient. In practice, we use a stochastic approximation of objective, drawing a mini-batch of <span class="math">\(N&lt;&lt;\vert D\vert\)</span> instance, which is</p>
<div class="math">\begin{equation}L(W)\approx \frac{1}{N}\sum_{i}^{\vert N\vert}{f_w (X^{i})} +\lambda r(W)\end{equation}</div>
<p><br />
The model computes <span class="math">\(f_W\)</span> in the forward pass and the gradient <span class="math">\(\nabla f_W\)</span> in the backward pass.The parameter update <span class="math">\(\nabla W\)</span> is formed by the solver from the error gradient <span class="math">\(\nabla f_W\)</span>, the regularization gradient <span class="math">\(\nabla r(W)\)</span>, and other particulars to each method.</p>
<p>Stochastic gradient descent (solver_type: SGD) updates the weights<span class="math">\( W\)</span> by a linear combination of the negative gradient <span class="math">\(\nabla L(W)\)</span> and the previous weight update <span class="math">\(V_t\)</span>. The learning rate <span class="math">\(\alpha\)</span> is the weight of the negative gradient. The momentum <span class="math">\(\mu\)</span> is the weight of the previous update.</p>
<p>Formally, we have the following formulas to compute the update value <span class="math">\(V_{t+1}\)</span> and the updated weights <span class="math">\(W_{t+1}\)</span> at iteration <span class="math">\(t+1\)</span>, given the previous weight update <span class="math">\(V_t\)</span> and current weights <span class="math">\(W_t\)</span>:</p>
<div class="math">\begin{equation}\begin{aligned}V_{t+1}&amp;=\mu V_t -\alpha \nabla L(W_t) \\W_{t+1}&amp;=W_t+V_{t+1}\end{aligned}\end{equation}</div>
<p><br />
The learning “hyperparameters” (<span class="math">\(\alpha\)</span> and <span class="math">\(\mu\)</span>) might require a bit of tuning for best results. </p>
<p>The adaptive gradient (solver_type: ADAGRAD) method is a gradient-based optimization method (like SGD) that attempts to “find needles in haystacks in the form of very predictive but rarely seen features,” in Duchi et al.’s words. Given the update information from all previous iterations <span class="math">\((\nabla L(W))_{t'}\)</span> for <span class="math">\({t'}\in {1,2,...,t}\)</span>, the update formulas are as follows, specified for each component <span class="math">\(i\)</span> of the weights <span class="math">\(W\)</span>:</p>
<div class="math">\begin{equation}(W_{t+1})_i=(W_t)_i-\alpha \frac{(\nabla L(W_t))_i}{\sqrt{\sum_{t'=1}^{t}{(\nabla L(W_{t'}))^2_i}}}\end{equation}</div>
<p><br />
Note that in practice, for weights <span class="math">\(W\in \mathcal{R}^d\)</span>, AdaGrad implementations (including the one in Caffe) use only <span class="math">\(\mathcal{O}(d)\)</span> extra storage for the historical gradient information (rather than the <span class="math">\(\mathcal{O}(dt)\)</span> storage that would be necessary to store each historical gradient individually).</p>
<p>Nesterov’s accelerated gradient (solver_type: NESTEROV) was proposed by Nesterov  as an “optimal” method of convex optimization, achieving a convergence rate of <span class="math">\(\mathcal{O}(1/t^2)\)</span> rather than the <span class="math">\(\mathcal{O}(1/t)\)</span>. Though the required assumptions to achieve the <span class="math">\(\mathcal{O}(1/t^2)\)</span> convergence typically will not hold for deep networks trained with Caffe (e.g., due to non-smoothness and non-convexity), in practice NAG can be a very effective method for optimizing certain types of deep learning architectures, as demonstrated for deep MNIST autoencoders by Sutskever et al. </p>
<p>The weight update formulas look very similar to the SGD updates given above:</p>
<div class="math">\begin{equation}\begin{aligned}V_{t+1}&amp;=\mu V_t -\alpha \nabla L(W_t,\mu V_t)\\W_{t+1}&amp;=W_t+V_{t+1}\end{aligned}\end{equation}</div>
<p><br />
What distinguishes the method from SGD is the weight setting <span class="math">\(W\)</span> on which we compute the error gradient <span class="math">\(\nabla L(W)\)</span> – in NAG we take the gradient on weights with added momentum <span class="math">\(\nabla L(W_t+\mu V_t)\)</span>; in SGD we simply take the gradient <span class="math">\(\nabla L(W_t)\)</span> on the current weights themselves.</p>
<h1 id="767efa">The workflow of caffe</h1>
<h1 id="cc2808">network initiation</h1>
<p>Firstly, we construct all the network structure(which is essentially a <code>solver</code>) from a ptototxt file.</p>
<div class="codehilite"><pre><span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Solver</span><span class="p">(</span><span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">param_file</span><span class="p">)</span> <span class="o">:</span>
        <span class="n">net_</span><span class="p">()</span>
<span class="p">{</span>
    <span class="n">SolverParameter</span> <span class="n">param</span><span class="p">;</span>
    <span class="n">ReadProtoFromTextFileOrDie</span><span class="p">(</span><span class="n">param_file</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">param</span><span class="p">);</span>
    <span class="n">Init</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>The <code>ReadProtoFromTextFileOrDie</code> is a wrapper of protobuf, it would eventually execute</p>
<div class="codehilite"><pre><span class="kt">bool</span> <span class="n">success</span> <span class="o">=</span> <span class="n">google</span><span class="o">::</span><span class="n">protobuf</span><span class="o">::</span><span class="n">TextFormat</span><span class="o">::</span><span class="n">Parse</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">proto</span><span class="p">);</span>
</pre></div>


<p>So , don't bother yourself to dive into the mess of protobuf.</p>
<p>After we get all the parameters, we construct the nets,as such:</p>
<div class="codehilite"><pre><span class="kt">void</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Init</span><span class="p">(</span><span class="k">const</span> <span class="n">SolverParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span><span class="o">&lt;&lt;</span> <span class="s">&quot;Initializing solver from parameters: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span>
    <span class="o">&lt;&lt;</span> <span class="n">param</span><span class="p">.</span><span class="n">DebugString</span><span class="p">();</span>
    <span class="n">param_</span> <span class="o">=</span> <span class="n">param</span><span class="p">;</span>
    <span class="n">CHECK_GE</span><span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">average_loss</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;average_loss should be non-negative.&quot;</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">random_seed</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">Caffe</span><span class="o">::</span><span class="n">set_random_seed</span><span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">random_seed</span><span class="p">());</span>
    <span class="p">}</span>
    <span class="c1">// Scaffolding code</span>
    <span class="n">InitTrainNet</span><span class="p">();</span>
    <span class="n">InitTestNets</span><span class="p">();</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Solver scaffolding done.&quot;</span><span class="p">;</span>
    <span class="n">iter_</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">current_step_</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>


<p>In the <code>Init</code>, it calls two initiation functions <code>InitTrainNet</code> and <code>InitTrainNet</code>. Because the nets can be created by network parameter or a already-existed  network file(which has all the weights as well as network structure). So the <code>Init*Net</code> must differentiate these cases. Here is the <code>InitTrainNet</code> definition.</p>
<div class="codehilite"><pre><span class="kt">void</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">InitTrainNet</span><span class="p">()</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_train_nets</span> <span class="o">=</span> <span class="n">param_</span><span class="p">.</span><span class="n">has_net</span><span class="p">()</span> <span class="o">+</span> <span class="n">param_</span><span class="p">.</span><span class="n">has_net_param</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">param_</span><span class="p">.</span><span class="n">has_train_net</span><span class="p">()</span> <span class="o">+</span> <span class="n">param_</span><span class="p">.</span><span class="n">has_train_net_param</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">field_names</span> <span class="o">=</span> <span class="s">&quot;net, net_param, train_net, train_net_param&quot;</span><span class="p">;</span>
    <span class="n">CHECK_GE</span><span class="p">(</span><span class="n">num_train_nets</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">&lt;&lt;</span> <span class="s">&quot;SolverParameter must specify a train net &quot;</span>
    <span class="o">&lt;&lt;</span> <span class="s">&quot;using one of these fields: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">field_names</span><span class="p">;</span>
    <span class="n">CHECK_LE</span><span class="p">(</span><span class="n">num_train_nets</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">&lt;&lt;</span> <span class="s">&quot;SolverParameter must not contain more than &quot;</span>
    <span class="o">&lt;&lt;</span> <span class="s">&quot;one of these fields specifying a train_net: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">field_names</span><span class="p">;</span>
    <span class="n">NetParameter</span> <span class="n">net_param</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">has_train_net_param</span><span class="p">())</span>
    <span class="p">{</span>
        <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span><span class="o">&lt;&lt;</span> <span class="s">&quot;Creating training net specified in train_net_param.&quot;</span><span class="p">;</span>
        <span class="n">net_param</span><span class="p">.</span><span class="n">CopyFrom</span><span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">train_net_param</span><span class="p">());</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">has_train_net</span><span class="p">())</span>
    <span class="p">{</span>
        <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Creating training net from train_net file: &quot;</span>
        <span class="o">&lt;&lt;</span> <span class="n">param_</span><span class="p">.</span><span class="n">train_net</span><span class="p">();</span>
        <span class="n">ReadNetParamsFromTextFileOrDie</span><span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">train_net</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">net_param</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">has_net_param</span><span class="p">())</span>
    <span class="p">{</span>
        <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span><span class="o">&lt;&lt;</span> <span class="s">&quot;Creating training net specified in net_param.&quot;</span><span class="p">;</span>
        <span class="n">net_param</span><span class="p">.</span><span class="n">CopyFrom</span><span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">net_param</span><span class="p">());</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">has_net</span><span class="p">())</span>
    <span class="p">{</span>
        <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span><span class="o">&lt;&lt;</span> <span class="s">&quot;Creating training net from net file: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">param_</span><span class="p">.</span><span class="n">net</span><span class="p">();</span>
        <span class="n">ReadNetParamsFromTextFileOrDie</span><span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">net</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">net_param</span><span class="p">);</span>
    <span class="p">}</span>
        <span class="c1">// Set the correct NetState.  We start with the solver defaults (lowest</span>
        <span class="c1">// precedence); then, merge in any NetState specified by the net_param itself;</span>
        <span class="c1">// finally, merge in any NetState specified by the train_state (highest</span>
        <span class="c1">// precedence).</span>
    <span class="n">NetState</span> <span class="n">net_state</span><span class="p">;</span>
    <span class="n">net_state</span><span class="p">.</span><span class="n">set_phase</span><span class="p">(</span><span class="n">TRAIN</span><span class="p">);</span>
    <span class="n">net_state</span><span class="p">.</span><span class="n">MergeFrom</span><span class="p">(</span><span class="n">net_param</span><span class="p">.</span><span class="n">state</span><span class="p">());</span>
    <span class="n">net_state</span><span class="p">.</span><span class="n">MergeFrom</span><span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">train_state</span><span class="p">());</span>
    <span class="n">net_param</span><span class="p">.</span><span class="n">mutable_state</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">CopyFrom</span><span class="p">(</span><span class="n">net_state</span><span class="p">);</span>
    <span class="n">net_</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">Net</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">net_param</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>


<p>In the definition body, all jobs are done by protobuf.</p>
<h1 id="fa9de3">network trainning</h1>
<p>The network trainning is a nondetermistic procedure, because the trainning phase would run forever. To choose the right time to end the trainning phase. Usually, there is one simple principle :we set a stop_iter that we can tolerate  to limit the iteration times. So in the <code>Solve</code> function, which is the caffe execution body, it sets the iteration limit then calls the step function to do the real work. </p>
<div class="codehilite"><pre><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Step</span><span class="p">(</span><span class="kt">int</span> <span class="n">iters</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;</span> <span class="n">bottom_vec</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">start_iter</span> <span class="o">=</span> <span class="n">iter_</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">stop_iter</span> <span class="o">=</span> <span class="n">iter_</span> <span class="o">+</span> <span class="n">iters</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">average_loss</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">param_</span><span class="p">.</span><span class="n">average_loss</span><span class="p">();</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="n">losses</span><span class="p">;</span>
    <span class="n">Dtype</span> <span class="n">smoothed_loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(;</span> <span class="n">iter_</span> <span class="o">&lt;</span> <span class="n">stop_iter</span><span class="p">;</span> <span class="o">++</span><span class="n">iter_</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">test_interval</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">iter_</span> <span class="o">%</span> <span class="n">param_</span><span class="p">.</span><span class="n">test_interval</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">iter_</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">param_</span><span class="p">.</span><span class="n">test_initialization</span><span class="p">()))</span>
        <span class="p">{</span>
            <span class="n">TestAll</span><span class="p">();</span>
        <span class="p">}</span>

        <span class="k">const</span> <span class="kt">bool</span> <span class="n">display</span> <span class="o">=</span> <span class="n">param_</span><span class="p">.</span><span class="n">display</span><span class="p">()</span>
                <span class="o">&amp;&amp;</span> <span class="n">iter_</span> <span class="o">%</span> <span class="n">param_</span><span class="p">.</span><span class="n">display</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
        <span class="n">net_</span><span class="o">-&gt;</span><span class="n">set_debug_info</span><span class="p">(</span><span class="n">display</span> <span class="o">&amp;&amp;</span> <span class="n">param_</span><span class="p">.</span><span class="n">debug_info</span><span class="p">());</span>
        <span class="n">Dtype</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">net_</span><span class="o">-&gt;</span><span class="n">ForwardBackward</span><span class="p">(</span><span class="n">bottom_vec</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">average_loss</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">losses</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">loss</span><span class="p">);</span>
            <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">losses</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
            <span class="n">smoothed_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">smoothed_loss</span> <span class="o">*</span> <span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">size</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">else</span>
        <span class="p">{</span>
            <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">iter_</span> <span class="o">-</span> <span class="n">start_iter</span><span class="p">)</span> <span class="o">%</span> <span class="n">average_loss</span><span class="p">;</span>
            <span class="n">smoothed_loss</span> <span class="o">+=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">-</span> <span class="n">losses</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">/</span> <span class="n">average_loss</span><span class="p">;</span>
            <span class="n">losses</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">display</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="c1">//some code to display some information of current iteration, like loss </span>
        <span class="p">}</span>
        <span class="n">ComputeUpdateValue</span><span class="p">();</span>
        <span class="n">net_</span><span class="o">-&gt;</span><span class="n">Update</span><span class="p">();</span>

        <span class="c1">// Save a snapshot if needed.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">snapshot</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">iter_</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">param_</span><span class="p">.</span><span class="n">snapshot</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">Snapshot</span><span class="p">();</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>Every iteration, caffe</p>
<ul>
<li>
<p>issues a test to all the test files every <code>test_interval</code> iterations, output some test info</p>
</li>
<li>
<p>calls <code>net_-&gt;ForwardBackward</code> to train a batch, which would call chained <code>layer-&gt;Forward</code> and <code>layer-&gt;Backward</code> eventually.</p>
</li>
<li>
<p>calls <code>ComputeUpdateValue</code> to set all the diffs of blobs. This functions is virtual, its implemetation depends on the specific <code>Solver</code>.</p>
</li>
<li>
<p>calls <code>net_-&gt;Update</code> to update the weight.</p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
    <div id="content-comments">
<div id="disqus_thread"></div>
<script type="text/javascript">var disqus_shortname="SpiritsAway";(function(){var a=document.createElement("script");a.type="text/javascript";a.async=true;a.src="//"+disqus_shortname+".disqus.com/embed.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)})();</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

</div>
<div class="niu2-right-container col-md-2">
    <div id="niu2-sidebar-meta" class="niu2-sidebar">
        <div class="niu2-sidebar-label"><i class="icon-calendar"></i>发布时间:</div>
        <div class="niu2-sidebar-value">2015-04-20 21:31</div>
        <div class="niu2-sidebar-label"><i class="icon-open-folder"></i>分类:</div>
        <div class="niu2-sidebar-value"><a href="http://spiritsaway.info/code/index.html">Code</a></div>
        <div class="niu2-sidebar-label"><i class="icon-tag"></i>标签:</div>
 
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="http://spiritsaway.info/tag/nn.html">NN</a><sup>2</sup></div>
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="http://spiritsaway.info/tag/code.html">Code</a><sup>3</sup></div>
            <div class="niu2-sidebar-inter-value niu2-sidebar-tag"><a href="http://spiritsaway.info/tag/cpp.html">Cpp</a><sup>5</sup></div>
    </div>

    <div id="niu2-sidebar-toc" class="niu2-sidebar" data-status="closed">
        <div class="niu2-sidebar-label">
            <i id="niu2-sidebar-toc-ctrl" class="icon-open-tocs"></i>目录
        </div>
        <ol id="niu2-sidebar-toc-list">
            <li><a href="#content-heading">Under the Hood : Caffe</a></li>
            <li><a href='#9fc63e'>device_alternate.hpp</a></li><li><a href='#0e4f38'>proto/caffe.pb.h</a></li><li><a href='#23937f'>util/rng.hpp</a></li><li><a href='#ad7c65'>util/mkl_alternate.h</a></li><li><a href='#83fcbe'>util/math_fucntions</a></li><li><a href='#16cd7a'>common.hpp</a></li><li><a href='#552c74'>syncedmem.hpp</a></li><li><a href='#ed894f'>util/db.hpp</a></li><li><a href='#8e17f8'>util/io.hpp</a></li><li><a href='#e16c12'>internal_thread.hpp</a></li><li><a href='#f1872c'>The Main Structure of Caffe</a></li><li><a href='#a7d6d5'>blob.hpp</a></li><li><a href='#401b9c'>filler.hpp</a></li><li><a href='#e22d0b'>layer.hpp</a></li><li><a href='#4ae9e3'>net.hpp</a></li><li><a href='#c50011'>solver.hpp</a></li><li><a href='#767efa'>The workflow of caffe</a></li><li><a href='#cc2808'>network initiation</a></li><li><a href='#fa9de3'>network trainning</a></li>
            <li><a href="#content-comments">评论</a></li>
        </ol>
    </div>
</div>    </div>

    <div class="niu2-footer">
        <div id="body-footer" class="col-md-6 col-md-offset-2">
<hr/>
<p>
    Powered by <a href="https://github.com/getpelican/pelican">Pelican</a>, 
    <a href="https://github.com/mawenbao/niu-x2-sidebar">theme</a> built with <a href="http://getbootstrap.com">Bootstrap3</a>
    by <a href="http://blog.atime.me">Ma Wenbao</a>, icons by 
    <a href="http://fortawesome.github.io/Font-Awesome">Font Awesome</a>.
</p>
<p>
    ©
            2014-2015
    <a class="niu2-footer-link" href="http://spiritsaway.info">Huiliang Huang</a>
</p>
<p class="niu2-icons">
        <a class="niu2-footer-icon" href="mailto: spiritsaway@outlook.com" title="my email address">
            <i class="fa-envelope-alt icon-lg"></i>
        </a>
        <a class="niu2-footer-icon" href="https://github.com/huangfeidian" title="my github page">
            <i class="fa-github-alt icon-lg"></i>
        </a>
</p>        </div>
    </div>
    
    <div id="niu2-pygments" data-theme="github"></div>

    <script type="text/javascript" src="http://spiritsaway.info/theme/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="http://spiritsaway.info/theme/js/niu2.min.js"></script>
    <script type="text/javascript" src="http://spiritsaway.info/theme/js/bootstrap.min.js"></script>
    <script type="text/javascript">onContentLoaded();</script>
  </body>
</html>